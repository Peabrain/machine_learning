{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI(nn.Module):\n",
    "    def __init__(self,conv_kernel):\n",
    "        super(AI, self).__init__()\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.lstm_layers = 1\n",
    "        self.lstm_depth = 1000\n",
    "        self.cnn1 = nn.Conv2d(1,self.conv_kernel,5,stride=2,padding=1)\n",
    "        self.cnn1_2 = nn.Conv2d(self.conv_kernel,self.conv_kernel,3,stride=1)\n",
    "        self.cnn2 = nn.Conv2d(self.conv_kernel,self.conv_kernel,3,stride=2,padding=0)\n",
    "        self.rnn = nn.LSTM(5 * 5, self.lstm_depth,self.lstm_layers)\n",
    "        self.fc1 = nn.Linear(self.conv_kernel * self.lstm_depth,self.lstm_depth)\n",
    "        self.fc2 = nn.Linear(self.lstm_depth,10)\n",
    "        self.out = nn.Dropout(0.9)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')     \n",
    "        self.to(self.device)\n",
    "    def forward(self,x):\n",
    "        hx = T.zeros(self.lstm_layers,self.conv_kernel, self.lstm_depth).to(self.device)\n",
    "        cx = T.zeros(self.lstm_layers,self.conv_kernel, self.lstm_depth).to(self.device)\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn1_2(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = x.view(-1,self.conv_kernel,5 * 5)\n",
    "        hx, cx = self.rnn(x, (hx, cx))\n",
    "        x = hx.view(-1,self.conv_kernel * self.lstm_depth)\n",
    "        x = T.tanh(self.fc1(x))\n",
    "        x = T.sigmoid(self.fc2(x))\n",
    "#        print(o.shape)\n",
    "#        o = self.out(o)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLAYFIELD():\n",
    "    def __init__(self,width,height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pf = np.zeros((self.width,self.height,2))\n",
    "    def setStone(self,x,y,stone):\n",
    "        s = np.zeros(2)\n",
    "        s[stone] = 1\n",
    "        self.pf[y][x] = s\n",
    "    def get(self):\n",
    "        return self.pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:2: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:4: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "with open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "with open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_labels = extract_labels(f)\n",
    "\n",
    "with open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "with open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = extract_labels(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 loss: 0.002555728256702423\n",
      "Step: 100 loss: 0.0659104516170919\n",
      "Step: 200 loss: 0.03750291411415674\n",
      "Step: 300 loss: 0.03371536461636424\n",
      "Step: 400 loss: 0.025494956957409158\n",
      "Step: 500 loss: 0.02888321423874004\n",
      "Step: 600 loss: 0.023229636772011873\n",
      "Step: 700 loss: 0.02121426889265422\n",
      "Step: 800 loss: 0.016647743974026526\n",
      "Step: 900 loss: 0.023323459564708174\n",
      "Step: 1000 loss: 0.017756924685527337\n",
      "Step: 1100 loss: 0.022209286977085865\n",
      "Step: 1200 loss: 0.020750521204972755\n",
      "Step: 1300 loss: 0.019313499630370642\n",
      "Step: 1400 loss: 0.017268661420457646\n",
      "Step: 1500 loss: 0.010340712676770636\n",
      "Step: 1600 loss: 0.014461592787884002\n",
      "Step: 1700 loss: 0.01070790683676023\n",
      "Step: 1800 loss: 0.005780514274047164\n",
      "Step: 1900 loss: 0.009259243803899152\n",
      "Step: 2000 loss: 0.013625237210035265\n",
      "Step: 2100 loss: 0.014043649158338667\n",
      "Step: 2200 loss: 0.0074955815706562135\n",
      "Step: 2300 loss: 0.00601973836272009\n",
      "Step: 2400 loss: 0.007971899815129291\n",
      "Step: 2500 loss: 0.011990603665872186\n",
      "Step: 2600 loss: 0.006148691893267824\n",
      "Step: 2700 loss: 0.011544682580697553\n",
      "Step: 2800 loss: 0.009902290064073895\n",
      "Step: 2900 loss: 0.009965243885326346\n",
      "Step: 3000 loss: 0.013205726911201055\n",
      "Step: 3100 loss: 0.012006123506653239\n",
      "Step: 3200 loss: 0.0045612181247452095\n",
      "Step: 3300 loss: 0.012305080321602872\n",
      "Step: 3400 loss: 0.0070834324642055435\n",
      "Step: 3500 loss: 0.005824629437865951\n",
      "Step: 3600 loss: 0.006262412723972374\n",
      "Step: 3700 loss: 0.010932579725485994\n",
      "Step: 3800 loss: 0.01122864758810465\n",
      "Step: 3900 loss: 0.007116969069684274\n",
      "Step: 4000 loss: 0.005556939904417959\n",
      "Step: 4100 loss: 0.007589682423804334\n",
      "Step: 4200 loss: 0.008980866800211516\n",
      "Step: 4300 loss: 0.00662733660223239\n",
      "Step: 4400 loss: 0.008829113964493445\n",
      "Step: 4500 loss: 0.0072738849817324085\n",
      "Step: 4600 loss: 0.006640729885411929\n",
      "Step: 4700 loss: 0.015198022571084949\n",
      "Step: 4800 loss: 0.008419411166059945\n",
      "Step: 4900 loss: 0.0048222336564504075\n",
      "Step: 5000 loss: 0.007746633307224329\n",
      "Step: 5100 loss: 0.0055587216417916354\n",
      "Step: 5200 loss: 0.00969216633518954\n",
      "Step: 5300 loss: 0.004594597889654324\n",
      "Step: 5400 loss: 0.0066575018152343545\n",
      "Step: 5500 loss: 0.005717257624082777\n",
      "Step: 5600 loss: 0.007334909595901991\n",
      "Step: 5700 loss: 0.007044458785285315\n",
      "Step: 5800 loss: 0.0068554856851733344\n",
      "Step: 5900 loss: 0.011335543944678648\n",
      "Step: 6000 loss: 0.005031837392780289\n",
      "Step: 6100 loss: 0.005267590433650184\n",
      "Step: 6200 loss: 0.0028123121236149017\n",
      "Step: 6300 loss: 0.007857924594868565\n",
      "Step: 6400 loss: 0.004530656093829748\n",
      "Step: 6500 loss: 0.00818735347810616\n",
      "Step: 6600 loss: 0.00407936545897428\n",
      "Step: 6700 loss: 0.005655390693491426\n",
      "Step: 6800 loss: 0.003035915363852837\n",
      "Step: 6900 loss: 0.012243229479317962\n",
      "Step: 7000 loss: 0.0065693248065053925\n",
      "Step: 7100 loss: 0.008520058946787685\n",
      "Step: 7200 loss: 0.00633956738890447\n",
      "Step: 7300 loss: 0.012831817795731695\n",
      "Step: 7400 loss: 0.005801611933029562\n",
      "Step: 7500 loss: 0.005337973924806647\n",
      "Step: 7600 loss: 0.008016502468149157\n",
      "Step: 7700 loss: 0.006792333972407505\n",
      "Step: 7800 loss: 0.008348555419047443\n",
      "Step: 7900 loss: 0.008902676605575834\n",
      "Step: 8000 loss: 0.007173679327006539\n",
      "Step: 8100 loss: 0.004761002255095264\n",
      "Step: 8200 loss: 0.008161568698901646\n",
      "Step: 8300 loss: 0.011545819791253962\n",
      "Step: 8400 loss: 0.0032846646863163185\n",
      "Step: 8500 loss: 0.009107135752119576\n",
      "Step: 8600 loss: 0.0018853362952449971\n",
      "Step: 8700 loss: 0.010334659344207466\n",
      "Step: 8800 loss: 0.01362985856620071\n",
      "Step: 8900 loss: 0.00923547673844041\n",
      "Step: 9000 loss: 0.007589488753196747\n",
      "Step: 9100 loss: 0.004714716842240705\n",
      "Step: 9200 loss: 0.00794340293303776\n",
      "Step: 9300 loss: 0.006954180561622252\n",
      "Step: 9400 loss: 0.007597214704710495\n",
      "Step: 9500 loss: 0.007581024088926825\n",
      "Step: 9600 loss: 0.004679663397355398\n",
      "Step: 9700 loss: 0.0006026045899716337\n",
      "Step: 9800 loss: 0.0059112157136587485\n",
      "Step: 9900 loss: 0.0029394518557819536\n",
      "Step: 10000 loss: 0.0009225017703647609\n",
      "Step: 10100 loss: 0.006385844035735317\n",
      "Step: 10200 loss: 0.004533278406511272\n",
      "Step: 10300 loss: 0.011006671399604784\n",
      "Step: 10400 loss: 0.003703662007228559\n",
      "Step: 10500 loss: 0.0009646561122326603\n",
      "Step: 10600 loss: 0.00030953098176269124\n",
      "Step: 10700 loss: 0.0024806016791217187\n",
      "Step: 10800 loss: 0.010387911433958834\n",
      "Step: 10900 loss: 0.0039930099190110015\n",
      "Step: 11000 loss: 0.00553766140496009\n",
      "Step: 11100 loss: 0.005835844621362867\n",
      "Step: 11200 loss: 0.003274102365978706\n",
      "Step: 11300 loss: 0.008471189093966132\n",
      "Step: 11400 loss: 0.005901888623670856\n",
      "Step: 11500 loss: 0.002765375787971607\n",
      "Step: 11600 loss: 0.010164124707625889\n",
      "Step: 11700 loss: 0.004793704053986403\n",
      "Step: 11800 loss: 0.007498967202932363\n",
      "Step: 11900 loss: 0.00332241760290799\n",
      "Step: 12000 loss: 0.004147778787721563\n",
      "Step: 12100 loss: 0.003503686924352678\n",
      "Step: 12200 loss: 0.0007354313003452262\n",
      "Step: 12300 loss: 0.006265352967525359\n",
      "Step: 12400 loss: 0.005341086476237251\n",
      "Step: 12500 loss: 0.0026209327575020323\n",
      "Step: 12600 loss: 0.00518027854940101\n",
      "Step: 12700 loss: 0.006089077710430502\n",
      "Step: 12800 loss: 0.004659911802150418\n",
      "Step: 12900 loss: 0.005287643287229003\n",
      "Step: 13000 loss: 0.006330142547037667\n",
      "Step: 13100 loss: 0.0062421982414434755\n",
      "Step: 13200 loss: 0.00501524493184661\n",
      "Step: 13300 loss: 0.0016095323437866683\n",
      "Step: 13400 loss: 0.00486386968611896\n",
      "Step: 13500 loss: 0.0039598849670164785\n",
      "Step: 13600 loss: 0.0041967247539287204\n",
      "Step: 13700 loss: 0.003876324028278759\n",
      "Step: 13800 loss: 0.007260542852843627\n",
      "Step: 13900 loss: 0.0016949775505941034\n",
      "Step: 14000 loss: 0.004657547691685977\n",
      "Step: 14100 loss: 0.0043710311573090625\n",
      "Step: 14200 loss: 0.003535521814715139\n",
      "Step: 14300 loss: 0.007846088708008665\n",
      "Step: 14400 loss: 0.00658077874499213\n",
      "Step: 14500 loss: 0.0014762181258652163\n",
      "Step: 14600 loss: 0.005058381556748373\n",
      "Step: 14700 loss: 0.0035727692198679506\n",
      "Step: 14800 loss: 0.007324547624234583\n",
      "Step: 14900 loss: 0.0021834926330325287\n",
      "Step: 15000 loss: 0.0017948835279287324\n",
      "Step: 15100 loss: 0.0031174366861080214\n",
      "Step: 15200 loss: 0.005804181690871246\n",
      "Step: 15300 loss: 0.0029925257911509106\n",
      "Step: 15400 loss: 0.005255656979352352\n",
      "Step: 15500 loss: 0.008257690804703089\n",
      "Step: 15600 loss: 0.002148017400611479\n",
      "Step: 15700 loss: 0.0020924849189248107\n",
      "Step: 15800 loss: 0.006755972030354087\n",
      "Step: 15900 loss: 0.007413949866670464\n",
      "Step: 16000 loss: 0.00519552757887368\n",
      "Step: 16100 loss: 0.005096140782738985\n",
      "Step: 16200 loss: 0.0028921491206870087\n",
      "Step: 16300 loss: 0.0027189614856592925\n",
      "Step: 16400 loss: 0.0008235717191473668\n",
      "Step: 16500 loss: 0.002706831729944952\n",
      "Step: 16600 loss: 0.0027481443746683\n",
      "Step: 16700 loss: 0.00427408542394005\n",
      "Step: 16800 loss: 0.0044853433783282525\n",
      "Step: 16900 loss: 0.002741106266088309\n",
      "Step: 17000 loss: 0.006041017516349711\n",
      "Step: 17100 loss: 0.005647886964566169\n",
      "Step: 17200 loss: 0.004697936163486247\n",
      "Step: 17300 loss: 0.005979076785223469\n",
      "Step: 17400 loss: 0.0011655713074549112\n",
      "Step: 17500 loss: 0.004885064205998333\n",
      "Step: 17600 loss: 0.009011358321514536\n",
      "Step: 17700 loss: 0.001212059492837625\n",
      "Step: 17800 loss: 0.006030017866822846\n",
      "Step: 17900 loss: 0.006383318951386627\n",
      "Step: 18000 loss: 0.005316587984867738\n",
      "Step: 18100 loss: 0.005167412842085923\n",
      "Step: 18200 loss: 0.0029246991714671823\n",
      "Step: 18300 loss: 0.0011208338421783992\n",
      "Step: 18400 loss: 0.004575931724966722\n",
      "Step: 18500 loss: 0.0025637271540892926\n",
      "Step: 18600 loss: 0.002348464075371339\n",
      "Step: 18700 loss: 0.0021772057564010084\n",
      "Step: 18800 loss: 0.003979800534480092\n",
      "Step: 18900 loss: 0.0023366171390318867\n",
      "Step: 19000 loss: 0.0009828504647555292\n",
      "Step: 19100 loss: 0.007148786361462953\n",
      "Step: 19200 loss: 0.00640180943056464\n",
      "Step: 19300 loss: 0.00429175853689344\n",
      "Step: 19400 loss: 0.00819868380713615\n",
      "Step: 19500 loss: 0.0025880102228143186\n",
      "Step: 19600 loss: 0.004470096090066136\n",
      "Step: 19700 loss: 0.0008405664346992126\n",
      "Step: 19800 loss: 0.0028297885941310596\n",
      "Step: 19900 loss: 0.005723535142061564\n",
      "Step: 20000 loss: 0.0011591944248175424\n",
      "Step: 20100 loss: 0.008442040075476599\n",
      "Step: 20200 loss: 0.0047799265731009654\n",
      "Step: 20300 loss: 0.004394886098823463\n",
      "Step: 20400 loss: 0.0033638648413949566\n",
      "Step: 20500 loss: 0.0004269339992345067\n",
      "Step: 20600 loss: 0.0032504890350548974\n",
      "Step: 20700 loss: 0.00431273209143626\n",
      "Step: 20800 loss: 0.00966016354921976\n",
      "Step: 20900 loss: 0.002891637683309227\n",
      "Step: 21000 loss: 0.004960016890561292\n",
      "Step: 21100 loss: 0.004157664168859583\n",
      "Step: 21200 loss: 0.005671185965468339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21300 loss: 0.0013352721837964054\n",
      "Step: 21400 loss: 0.0010247543468153708\n",
      "Step: 21500 loss: 0.0028146081238310215\n",
      "Step: 21600 loss: 0.001695754688640818\n",
      "Step: 21700 loss: 0.005052084001879393\n",
      "Step: 21800 loss: 0.0011780223243169985\n",
      "Step: 21900 loss: 0.0020647118575254806\n",
      "Step: 22000 loss: 0.0057001873093895485\n",
      "Step: 22100 loss: 0.0013549315787633987\n",
      "Step: 22200 loss: 0.005628394633305334\n",
      "Step: 22300 loss: 0.004978525052289342\n",
      "Step: 22400 loss: 0.002418164162336325\n",
      "Step: 22500 loss: 0.004431243413483799\n",
      "Step: 22600 loss: 0.004772911886655038\n",
      "Step: 22700 loss: 0.0036984093251908236\n",
      "Step: 22800 loss: 0.0040301870169503215\n",
      "Step: 22900 loss: 0.003052846887603664\n",
      "Step: 23000 loss: 0.00027257765454123727\n",
      "Step: 23100 loss: 0.004465306606211925\n",
      "Step: 23200 loss: 0.002775338232058857\n",
      "Step: 23300 loss: 0.0008886950139412875\n",
      "Step: 23400 loss: 0.0035096258180249153\n",
      "Step: 23500 loss: 0.0050866829056127475\n",
      "Step: 23600 loss: 0.0029627092353007355\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ai_conv_kernel = 32\n",
    "ai = AI(ai_conv_kernel)\n",
    "\n",
    "max_mem = 1000\n",
    "a = 28\n",
    "\n",
    "inp = []\n",
    "for i in range(max_mem):\n",
    "    inp.append(np.random.randn(1, a, 28))\n",
    "    \n",
    "criterion = nn.MSELoss()#SmoothL1Loss()MSELoss\n",
    "optimizer = optim.RMSprop(ai.parameters(), lr=0.00005)\n",
    "\n",
    "losses = 0\n",
    "for j in range(1):\n",
    "    for i in range(60000):#train_images.shape[0]):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            inp1 = train_images[i]#inp[i % max_mem].copy() + np.random.randn(1, a, 28) * 0.1\n",
    "            inp1.shape = (1,1,28,28)\n",
    "            inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "            out = ai(inp1)\n",
    "    #        print(out)\n",
    "    #        print(target)\n",
    "#            target = out.cpu().detach().numpy().copy()\n",
    "#            print(target.shape)\n",
    "#            target[0][8 - 1] = np.zeros(10)\n",
    "#            target[0][8 - 1][train_labels[i]] = 1.0\n",
    "\n",
    "#            target[0] = np.zeros([1,8,10])\n",
    "            target = np.zeros(10)\n",
    "            target[train_labels[i]] = 1.0\n",
    "#            for o in range(ai_conv_kernel):\n",
    "#                oo = (o + 1) / ai_conv_kernel\n",
    "#                oo = oo * oo\n",
    "#                target[0][o] = oo * n + (1.0 - oo) * target[0][o]\n",
    "#            target[0][8 - 1][train_labels[i]] = 1.0\n",
    "#            print(target)\n",
    "            target = T.from_numpy(np.array(target)).to(ai.device)\n",
    "            loss = criterion(out, target.float())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        losses = losses + loss.item()\n",
    "        if (i % 100) == 0:\n",
    "            print('Step: ' + str(i) + ' loss: ' + str(losses / 100))\n",
    "            losses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(10000):#train_images.shape[0]):\n",
    "    optimizer.zero_grad()\n",
    "    inp1 = test_images[i]#inp[i % max_mem].copy()\n",
    "    inp1.shape = (1,1,28,28)\n",
    "    inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "    out = ai(inp1).cpu().detach().numpy()\n",
    "    m = np.argmax(out)\n",
    "    if m == test_labels[i]:\n",
    "#        print(m)\n",
    "        count = count + 1\n",
    "print(\"correct: \" + str(count / 10000))#train_images.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
