{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI(nn.Module):\n",
    "    def __init__(self,conv_kernel):\n",
    "        super(AI, self).__init__()\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.lstm_layers = 1\n",
    "        self.lstm_depth = 1000\n",
    "        self.cnn1 = nn.Conv2d(1,self.conv_kernel,5,stride=2,padding=1)\n",
    "        self.cnn1_2 = nn.Conv2d(self.conv_kernel,self.conv_kernel,3,stride=1)\n",
    "        self.cnn2 = nn.Conv2d(self.conv_kernel,self.conv_kernel,3,stride=2,padding=0)\n",
    "        self.rnn = nn.LSTM(5 * 5, self.lstm_depth,self.lstm_layers)\n",
    "        self.fc1 = nn.Linear(self.conv_kernel * self.lstm_depth,self.lstm_depth)\n",
    "        self.fc2 = nn.Linear(self.lstm_depth,10)\n",
    "        self.out = nn.Dropout(0.9)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')     \n",
    "        self.to(self.device)\n",
    "    def forward(self,x):\n",
    "        hx = T.zeros(self.lstm_layers,self.conv_kernel, self.lstm_depth).to(self.device)\n",
    "        cx = T.zeros(self.lstm_layers,self.conv_kernel, self.lstm_depth).to(self.device)\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn1_2(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = x.view(-1,self.conv_kernel,5 * 5)\n",
    "        hx, cx = self.rnn(x, (hx, cx))\n",
    "        x = hx.view(-1,self.conv_kernel * self.lstm_depth)\n",
    "        x = T.tanh(self.fc1(x))\n",
    "        x = T.sigmoid(self.fc2(x))\n",
    "#        print(o.shape)\n",
    "#        o = self.out(o)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLAYFIELD():\n",
    "    def __init__(self,width,height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pf = np.zeros((self.width,self.height,2))\n",
    "    def setStone(self,x,y,stone):\n",
    "        s = np.zeros(2)\n",
    "        s[stone] = 1\n",
    "        self.pf[y][x] = s\n",
    "    def get(self):\n",
    "        return self.pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:2: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:4: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "with open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "with open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_labels = extract_labels(f)\n",
    "\n",
    "with open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "with open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = extract_labels(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 loss: 0.002555728256702423\n",
      "Step: 100 loss: 0.0659104516170919\n",
      "Step: 200 loss: 0.03750291411415674\n",
      "Step: 300 loss: 0.03371536461636424\n",
      "Step: 400 loss: 0.025494956957409158\n",
      "Step: 500 loss: 0.02888321423874004\n",
      "Step: 600 loss: 0.023229636772011873\n",
      "Step: 700 loss: 0.02121426889265422\n",
      "Step: 800 loss: 0.016647743974026526\n",
      "Step: 900 loss: 0.023323459564708174\n",
      "Step: 1000 loss: 0.017756924685527337\n",
      "Step: 1100 loss: 0.022209286977085865\n",
      "Step: 1200 loss: 0.020750521204972755\n",
      "Step: 1300 loss: 0.019313499630370642\n",
      "Step: 1400 loss: 0.017268661420457646\n",
      "Step: 1500 loss: 0.010340712676770636\n",
      "Step: 1600 loss: 0.014461592787884002\n",
      "Step: 1700 loss: 0.01070790683676023\n",
      "Step: 1800 loss: 0.005780514274047164\n",
      "Step: 1900 loss: 0.009259243803899152\n",
      "Step: 2000 loss: 0.013625237210035265\n",
      "Step: 2100 loss: 0.014043649158338667\n",
      "Step: 2200 loss: 0.0074955815706562135\n",
      "Step: 2300 loss: 0.00601973836272009\n",
      "Step: 2400 loss: 0.007971899815129291\n",
      "Step: 2500 loss: 0.011990603665872186\n",
      "Step: 2600 loss: 0.006148691893267824\n",
      "Step: 2700 loss: 0.011544682580697553\n",
      "Step: 2800 loss: 0.009902290064073895\n",
      "Step: 2900 loss: 0.009965243885326346\n",
      "Step: 3000 loss: 0.013205726911201055\n",
      "Step: 3100 loss: 0.012006123506653239\n",
      "Step: 3200 loss: 0.0045612181247452095\n",
      "Step: 3300 loss: 0.012305080321602872\n",
      "Step: 3400 loss: 0.0070834324642055435\n",
      "Step: 3500 loss: 0.005824629437865951\n",
      "Step: 3600 loss: 0.006262412723972374\n",
      "Step: 3700 loss: 0.010932579725485994\n",
      "Step: 3800 loss: 0.01122864758810465\n",
      "Step: 3900 loss: 0.007116969069684274\n",
      "Step: 4000 loss: 0.005556939904417959\n",
      "Step: 4100 loss: 0.007589682423804334\n",
      "Step: 4200 loss: 0.008980866800211516\n",
      "Step: 4300 loss: 0.00662733660223239\n",
      "Step: 4400 loss: 0.008829113964493445\n",
      "Step: 4500 loss: 0.0072738849817324085\n",
      "Step: 4600 loss: 0.006640729885411929\n",
      "Step: 4700 loss: 0.015198022571084949\n",
      "Step: 4800 loss: 0.008419411166059945\n",
      "Step: 4900 loss: 0.0048222336564504075\n",
      "Step: 5000 loss: 0.007746633307224329\n",
      "Step: 5100 loss: 0.0055587216417916354\n",
      "Step: 5200 loss: 0.00969216633518954\n",
      "Step: 5300 loss: 0.004594597889654324\n",
      "Step: 5400 loss: 0.0066575018152343545\n",
      "Step: 5500 loss: 0.005717257624082777\n",
      "Step: 5600 loss: 0.007334909595901991\n",
      "Step: 5700 loss: 0.007044458785285315\n",
      "Step: 5800 loss: 0.0068554856851733344\n",
      "Step: 5900 loss: 0.011335543944678648\n",
      "Step: 6000 loss: 0.005031837392780289\n",
      "Step: 6100 loss: 0.005267590433650184\n",
      "Step: 6200 loss: 0.0028123121236149017\n",
      "Step: 6300 loss: 0.007857924594868565\n",
      "Step: 6400 loss: 0.004530656093829748\n",
      "Step: 6500 loss: 0.00818735347810616\n",
      "Step: 6600 loss: 0.00407936545897428\n",
      "Step: 6700 loss: 0.005655390693491426\n",
      "Step: 6800 loss: 0.003035915363852837\n",
      "Step: 6900 loss: 0.012243229479317962\n",
      "Step: 7000 loss: 0.0065693248065053925\n",
      "Step: 7100 loss: 0.008520058946787685\n",
      "Step: 7200 loss: 0.00633956738890447\n",
      "Step: 7300 loss: 0.012831817795731695\n",
      "Step: 7400 loss: 0.005801611933029562\n",
      "Step: 7500 loss: 0.005337973924806647\n",
      "Step: 7600 loss: 0.008016502468149157\n",
      "Step: 7700 loss: 0.006792333972407505\n",
      "Step: 7800 loss: 0.008348555419047443\n",
      "Step: 7900 loss: 0.008902676605575834\n",
      "Step: 8000 loss: 0.007173679327006539\n",
      "Step: 8100 loss: 0.004761002255095264\n",
      "Step: 8200 loss: 0.008161568698901646\n",
      "Step: 8300 loss: 0.011545819791253962\n",
      "Step: 8400 loss: 0.0032846646863163185\n",
      "Step: 8500 loss: 0.009107135752119576\n",
      "Step: 8600 loss: 0.0018853362952449971\n",
      "Step: 8700 loss: 0.010334659344207466\n",
      "Step: 8800 loss: 0.01362985856620071\n",
      "Step: 8900 loss: 0.00923547673844041\n",
      "Step: 9000 loss: 0.007589488753196747\n",
      "Step: 9100 loss: 0.004714716842240705\n",
      "Step: 9200 loss: 0.00794340293303776\n",
      "Step: 9300 loss: 0.006954180561622252\n",
      "Step: 9400 loss: 0.007597214704710495\n",
      "Step: 9500 loss: 0.007581024088926825\n",
      "Step: 9600 loss: 0.004679663397355398\n",
      "Step: 9700 loss: 0.0006026045899716337\n",
      "Step: 9800 loss: 0.0059112157136587485\n",
      "Step: 9900 loss: 0.0029394518557819536\n",
      "Step: 10000 loss: 0.0009225017703647609\n",
      "Step: 10100 loss: 0.006385844035735317\n",
      "Step: 10200 loss: 0.004533278406511272\n",
      "Step: 10300 loss: 0.011006671399604784\n",
      "Step: 10400 loss: 0.003703662007228559\n",
      "Step: 10500 loss: 0.0009646561122326603\n",
      "Step: 10600 loss: 0.00030953098176269124\n",
      "Step: 10700 loss: 0.0024806016791217187\n",
      "Step: 10800 loss: 0.010387911433958834\n",
      "Step: 10900 loss: 0.0039930099190110015\n",
      "Step: 11000 loss: 0.00553766140496009\n",
      "Step: 11100 loss: 0.005835844621362867\n",
      "Step: 11200 loss: 0.003274102365978706\n",
      "Step: 11300 loss: 0.008471189093966132\n",
      "Step: 11400 loss: 0.005901888623670856\n",
      "Step: 11500 loss: 0.002765375787971607\n",
      "Step: 11600 loss: 0.010164124707625889\n",
      "Step: 11700 loss: 0.004793704053986403\n",
      "Step: 11800 loss: 0.007498967202932363\n",
      "Step: 11900 loss: 0.00332241760290799\n",
      "Step: 12000 loss: 0.004147778787721563\n",
      "Step: 12100 loss: 0.003503686924352678\n",
      "Step: 12200 loss: 0.0007354313003452262\n",
      "Step: 12300 loss: 0.006265352967525359\n",
      "Step: 12400 loss: 0.005341086476237251\n",
      "Step: 12500 loss: 0.0026209327575020323\n",
      "Step: 12600 loss: 0.00518027854940101\n",
      "Step: 12700 loss: 0.006089077710430502\n",
      "Step: 12800 loss: 0.004659911802150418\n",
      "Step: 12900 loss: 0.005287643287229003\n",
      "Step: 13000 loss: 0.006330142547037667\n",
      "Step: 13100 loss: 0.0062421982414434755\n",
      "Step: 13200 loss: 0.00501524493184661\n",
      "Step: 13300 loss: 0.0016095323437866683\n",
      "Step: 13400 loss: 0.00486386968611896\n",
      "Step: 13500 loss: 0.0039598849670164785\n",
      "Step: 13600 loss: 0.0041967247539287204\n",
      "Step: 13700 loss: 0.003876324028278759\n",
      "Step: 13800 loss: 0.007260542852843627\n",
      "Step: 13900 loss: 0.0016949775505941034\n",
      "Step: 14000 loss: 0.004657547691685977\n",
      "Step: 14100 loss: 0.0043710311573090625\n",
      "Step: 14200 loss: 0.003535521814715139\n",
      "Step: 14300 loss: 0.007846088708008665\n",
      "Step: 14400 loss: 0.00658077874499213\n",
      "Step: 14500 loss: 0.0014762181258652163\n",
      "Step: 14600 loss: 0.005058381556748373\n",
      "Step: 14700 loss: 0.0035727692198679506\n",
      "Step: 14800 loss: 0.007324547624234583\n",
      "Step: 14900 loss: 0.0021834926330325287\n",
      "Step: 15000 loss: 0.0017948835279287324\n",
      "Step: 15100 loss: 0.0031174366861080214\n",
      "Step: 15200 loss: 0.005804181690871246\n",
      "Step: 15300 loss: 0.0029925257911509106\n",
      "Step: 15400 loss: 0.005255656979352352\n",
      "Step: 15500 loss: 0.008257690804703089\n",
      "Step: 15600 loss: 0.002148017400611479\n",
      "Step: 15700 loss: 0.0020924849189248107\n",
      "Step: 15800 loss: 0.006755972030354087\n",
      "Step: 15900 loss: 0.007413949866670464\n",
      "Step: 16000 loss: 0.00519552757887368\n",
      "Step: 16100 loss: 0.005096140782738985\n",
      "Step: 16200 loss: 0.0028921491206870087\n",
      "Step: 16300 loss: 0.0027189614856592925\n",
      "Step: 16400 loss: 0.0008235717191473668\n",
      "Step: 16500 loss: 0.002706831729944952\n",
      "Step: 16600 loss: 0.0027481443746683\n",
      "Step: 16700 loss: 0.00427408542394005\n",
      "Step: 16800 loss: 0.0044853433783282525\n",
      "Step: 16900 loss: 0.002741106266088309\n",
      "Step: 17000 loss: 0.006041017516349711\n",
      "Step: 17100 loss: 0.005647886964566169\n",
      "Step: 17200 loss: 0.004697936163486247\n",
      "Step: 17300 loss: 0.005979076785223469\n",
      "Step: 17400 loss: 0.0011655713074549112\n",
      "Step: 17500 loss: 0.004885064205998333\n",
      "Step: 17600 loss: 0.009011358321514536\n",
      "Step: 17700 loss: 0.001212059492837625\n",
      "Step: 17800 loss: 0.006030017866822846\n",
      "Step: 17900 loss: 0.006383318951386627\n",
      "Step: 18000 loss: 0.005316587984867738\n",
      "Step: 18100 loss: 0.005167412842085923\n",
      "Step: 18200 loss: 0.0029246991714671823\n",
      "Step: 18300 loss: 0.0011208338421783992\n",
      "Step: 18400 loss: 0.004575931724966722\n",
      "Step: 18500 loss: 0.0025637271540892926\n",
      "Step: 18600 loss: 0.002348464075371339\n",
      "Step: 18700 loss: 0.0021772057564010084\n",
      "Step: 18800 loss: 0.003979800534480092\n",
      "Step: 18900 loss: 0.0023366171390318867\n",
      "Step: 19000 loss: 0.0009828504647555292\n",
      "Step: 19100 loss: 0.007148786361462953\n",
      "Step: 19200 loss: 0.00640180943056464\n",
      "Step: 19300 loss: 0.00429175853689344\n",
      "Step: 19400 loss: 0.00819868380713615\n",
      "Step: 19500 loss: 0.0025880102228143186\n",
      "Step: 19600 loss: 0.004470096090066136\n",
      "Step: 19700 loss: 0.0008405664346992126\n",
      "Step: 19800 loss: 0.0028297885941310596\n",
      "Step: 19900 loss: 0.005723535142061564\n",
      "Step: 20000 loss: 0.0011591944248175424\n",
      "Step: 20100 loss: 0.008442040075476599\n",
      "Step: 20200 loss: 0.0047799265731009654\n",
      "Step: 20300 loss: 0.004394886098823463\n",
      "Step: 20400 loss: 0.0033638648413949566\n",
      "Step: 20500 loss: 0.0004269339992345067\n",
      "Step: 20600 loss: 0.0032504890350548974\n",
      "Step: 20700 loss: 0.00431273209143626\n",
      "Step: 20800 loss: 0.00966016354921976\n",
      "Step: 20900 loss: 0.002891637683309227\n",
      "Step: 21000 loss: 0.004960016890561292\n",
      "Step: 21100 loss: 0.004157664168859583\n",
      "Step: 21200 loss: 0.005671185965468339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21300 loss: 0.0013352721837964054\n",
      "Step: 21400 loss: 0.0010247543468153708\n",
      "Step: 21500 loss: 0.0028146081238310215\n",
      "Step: 21600 loss: 0.001695754688640818\n",
      "Step: 21700 loss: 0.005052084001879393\n",
      "Step: 21800 loss: 0.0011780223243169985\n",
      "Step: 21900 loss: 0.0020647118575254806\n",
      "Step: 22000 loss: 0.0057001873093895485\n",
      "Step: 22100 loss: 0.0013549315787633987\n",
      "Step: 22200 loss: 0.005628394633305334\n",
      "Step: 22300 loss: 0.004978525052289342\n",
      "Step: 22400 loss: 0.002418164162336325\n",
      "Step: 22500 loss: 0.004431243413483799\n",
      "Step: 22600 loss: 0.004772911886655038\n",
      "Step: 22700 loss: 0.0036984093251908236\n",
      "Step: 22800 loss: 0.0040301870169503215\n",
      "Step: 22900 loss: 0.003052846887603664\n",
      "Step: 23000 loss: 0.00027257765454123727\n",
      "Step: 23100 loss: 0.004465306606211925\n",
      "Step: 23200 loss: 0.002775338232058857\n",
      "Step: 23300 loss: 0.0008886950139412875\n",
      "Step: 23400 loss: 0.0035096258180249153\n",
      "Step: 23500 loss: 0.0050866829056127475\n",
      "Step: 23600 loss: 0.0029627092353007355\n",
      "Step: 23700 loss: 0.004564551393305578\n",
      "Step: 23800 loss: 0.0037372600056005466\n",
      "Step: 23900 loss: 0.003142811181978686\n",
      "Step: 24000 loss: 0.006125840681663703\n",
      "Step: 24100 loss: 0.0024088886850324796\n",
      "Step: 24200 loss: 0.0028800926832241204\n",
      "Step: 24300 loss: 0.0033787937410431823\n",
      "Step: 24400 loss: 0.00022780275042009635\n",
      "Step: 24500 loss: 0.0031949275382078214\n",
      "Step: 24600 loss: 0.0021869510179044482\n",
      "Step: 24700 loss: 0.0062001458207294035\n",
      "Step: 24800 loss: 0.0043780688000651935\n",
      "Step: 24900 loss: 0.002806180793243982\n",
      "Step: 25000 loss: 0.0030851382907928836\n",
      "Step: 25100 loss: 0.002492491300273514\n",
      "Step: 25200 loss: 0.002883656095816036\n",
      "Step: 25300 loss: 0.002557663099888714\n",
      "Step: 25400 loss: 0.002627048132302434\n",
      "Step: 25500 loss: 0.00234172654875465\n",
      "Step: 25600 loss: 0.0042517884502856875\n",
      "Step: 25700 loss: 0.005551162088302135\n",
      "Step: 25800 loss: 0.006527710925597035\n",
      "Step: 25900 loss: 0.004069387925993908\n",
      "Step: 26000 loss: 0.005427472631213277\n",
      "Step: 26100 loss: 0.0017880681577344148\n",
      "Step: 26200 loss: 0.002318926527493659\n",
      "Step: 26300 loss: 0.0012163567663247932\n",
      "Step: 26400 loss: 0.006920782832634131\n",
      "Step: 26500 loss: 0.003160390060552345\n",
      "Step: 26600 loss: 0.006995170894402349\n",
      "Step: 26700 loss: 0.007724099938669724\n",
      "Step: 26800 loss: 0.00509023103079528\n",
      "Step: 26900 loss: 0.006752423298690928\n",
      "Step: 27000 loss: 0.0008603905766267416\n",
      "Step: 27100 loss: 0.003054396264813022\n",
      "Step: 27200 loss: 0.0046764610980142155\n",
      "Step: 27300 loss: 0.0037727396483086297\n",
      "Step: 27400 loss: 0.0016781612254374067\n",
      "Step: 27500 loss: 0.0021523935040261223\n",
      "Step: 27600 loss: 0.0077914777005923955\n",
      "Step: 27700 loss: 0.004929676905023541\n",
      "Step: 27800 loss: 0.002366608365513798\n",
      "Step: 27900 loss: 0.0025154752701524786\n",
      "Step: 28000 loss: 0.00112819923969937\n",
      "Step: 28100 loss: 0.00212417436101191\n",
      "Step: 28200 loss: 0.004802306306507944\n",
      "Step: 28300 loss: 0.0011531546527146475\n",
      "Step: 28400 loss: 0.004281998354480834\n",
      "Step: 28500 loss: 0.002647571145064944\n",
      "Step: 28600 loss: 0.003053163701393942\n",
      "Step: 28700 loss: 0.006496974427377609\n",
      "Step: 28800 loss: 0.004810014301116325\n",
      "Step: 28900 loss: 0.0005403695662676\n",
      "Step: 29000 loss: 0.004082610279786252\n",
      "Step: 29100 loss: 0.004398447216357795\n",
      "Step: 29200 loss: 0.005615341963486742\n",
      "Step: 29300 loss: 0.004807062142095617\n",
      "Step: 29400 loss: 0.004290298777603994\n",
      "Step: 29500 loss: 0.002249197573815991\n",
      "Step: 29600 loss: 0.0016797971643939037\n",
      "Step: 29700 loss: 0.0011017141137463682\n",
      "Step: 29800 loss: 0.0031600425181892433\n",
      "Step: 29900 loss: 0.002608215007434751\n",
      "Step: 30000 loss: 0.00364570965000496\n",
      "Step: 30100 loss: 0.004479643908657635\n",
      "Step: 30200 loss: 0.0034222835799664606\n",
      "Step: 30300 loss: 0.001705113983762203\n",
      "Step: 30400 loss: 0.0014693179281042034\n",
      "Step: 30500 loss: 0.00027069213800757553\n",
      "Step: 30600 loss: 0.003160810481043086\n",
      "Step: 30700 loss: 0.004303883727641278\n",
      "Step: 30800 loss: 0.0027932385619234877\n",
      "Step: 30900 loss: 0.005332876567400149\n",
      "Step: 31000 loss: 0.003377281276984263\n",
      "Step: 31100 loss: 0.0015032732658491455\n",
      "Step: 31200 loss: 0.006229943543640388\n",
      "Step: 31300 loss: 0.0016366467296870723\n",
      "Step: 31400 loss: 0.006624125322122154\n",
      "Step: 31500 loss: 0.0017589182188987706\n",
      "Step: 31600 loss: 0.003008060791908065\n",
      "Step: 31700 loss: 0.003939213709181786\n",
      "Step: 31800 loss: 0.006587719509213343\n",
      "Step: 31900 loss: 0.0020217959064370915\n",
      "Step: 32000 loss: 0.0031474791678988367\n",
      "Step: 32100 loss: 0.0033150020976609085\n",
      "Step: 32200 loss: 0.004084413533227292\n",
      "Step: 32300 loss: 0.004116944518462447\n",
      "Step: 32400 loss: 0.004431286877677394\n",
      "Step: 32500 loss: 0.006006566907723823\n",
      "Step: 32600 loss: 0.0026370455155654325\n",
      "Step: 32700 loss: 0.002771938815754993\n",
      "Step: 32800 loss: 0.0017861706893856422\n",
      "Step: 32900 loss: 0.006256959700021412\n",
      "Step: 33000 loss: 0.00134376670574909\n",
      "Step: 33100 loss: 0.0025318467760752127\n",
      "Step: 33200 loss: 0.000523953651555189\n",
      "Step: 33300 loss: 0.001873838198912381\n",
      "Step: 33400 loss: 0.004156231309746374\n",
      "Step: 33500 loss: 0.003695334209139105\n",
      "Step: 33600 loss: 0.0019007547615942145\n",
      "Step: 33700 loss: 0.0039477997659429324\n",
      "Step: 33800 loss: 0.00505841208374477\n",
      "Step: 33900 loss: 0.00020201448728016658\n",
      "Step: 34000 loss: 0.00019050713633532722\n",
      "Step: 34100 loss: 0.0030494166400663403\n",
      "Step: 34200 loss: 0.0004736880135396859\n",
      "Step: 34300 loss: 0.0005225315679675191\n",
      "Step: 34400 loss: 0.0016731920625318252\n",
      "Step: 34500 loss: 0.0056231198633201985\n",
      "Step: 34600 loss: 0.0030836422078766644\n",
      "Step: 34700 loss: 0.008197475469127937\n",
      "Step: 34800 loss: 0.003218637258637074\n",
      "Step: 34900 loss: 0.0066362243368237725\n",
      "Step: 35000 loss: 0.004236011913365587\n",
      "Step: 35100 loss: 0.0038581537591406345\n",
      "Step: 35200 loss: 0.0019284662836389543\n",
      "Step: 35300 loss: 0.0030204028562326357\n",
      "Step: 35400 loss: 0.002174291230408869\n",
      "Step: 35500 loss: 0.005566437926381411\n",
      "Step: 35600 loss: 0.0005496351826081991\n",
      "Step: 35700 loss: 0.003763479416172686\n",
      "Step: 35800 loss: 0.0023868012226225233\n",
      "Step: 35900 loss: 0.0034502857901108543\n",
      "Step: 36000 loss: 0.002891229172369094\n",
      "Step: 36100 loss: 0.002563931707998677\n",
      "Step: 36200 loss: 0.004570214340064069\n",
      "Step: 36300 loss: 0.0026543387103772887\n",
      "Step: 36400 loss: 0.000909445149869157\n",
      "Step: 36500 loss: 0.0036880559699858395\n",
      "Step: 36600 loss: 0.002928328010091832\n",
      "Step: 36700 loss: 0.0010694660338754147\n",
      "Step: 36800 loss: 0.005866408034380015\n",
      "Step: 36900 loss: 0.0035967534193642337\n",
      "Step: 37000 loss: 0.00202892073231709\n",
      "Step: 37100 loss: 0.007145820440023272\n",
      "Step: 37200 loss: 0.0032401351787228806\n",
      "Step: 37300 loss: 0.003962932636748348\n",
      "Step: 37400 loss: 0.004583993780791502\n",
      "Step: 37500 loss: 0.004642740925084183\n",
      "Step: 37600 loss: 0.0015096558540349746\n",
      "Step: 37700 loss: 0.0031689410923145546\n",
      "Step: 37800 loss: 0.004716204220993063\n",
      "Step: 37900 loss: 0.004779136903500784\n",
      "Step: 38000 loss: 0.0023375153137345705\n",
      "Step: 38100 loss: 0.0032600252961179875\n",
      "Step: 38200 loss: 0.00011991098965097535\n",
      "Step: 38300 loss: 0.0017023909479738108\n",
      "Step: 38400 loss: 0.004374773127252922\n",
      "Step: 38500 loss: 0.0018795748584622628\n",
      "Step: 38600 loss: 0.003199770798329098\n",
      "Step: 38700 loss: 0.0035262524325844423\n",
      "Step: 38800 loss: 0.002456245040055478\n",
      "Step: 38900 loss: 0.0009347695098230702\n",
      "Step: 39000 loss: 0.001244711885341303\n",
      "Step: 39100 loss: 0.0019336323644282062\n",
      "Step: 39200 loss: 0.002351989696243919\n",
      "Step: 39300 loss: 0.003014103216108879\n",
      "Step: 39400 loss: 0.008660237721840928\n",
      "Step: 39500 loss: 0.005649308786005633\n",
      "Step: 39600 loss: 0.002031831452446795\n",
      "Step: 39700 loss: 0.0045812991509512815\n",
      "Step: 39800 loss: 0.0026444601644868724\n",
      "Step: 39900 loss: 0.0037310372720773446\n",
      "Step: 40000 loss: 0.001144726531746585\n",
      "Step: 40100 loss: 0.0010099245701175618\n",
      "Step: 40200 loss: 0.00243930315250509\n",
      "Step: 40300 loss: 0.0019375850627034196\n",
      "Step: 40400 loss: 0.0041119396515563265\n",
      "Step: 40500 loss: 0.00011655993752057725\n",
      "Step: 40600 loss: 0.0029241161747681588\n",
      "Step: 40700 loss: 0.0029492166388680376\n",
      "Step: 40800 loss: 0.0029461470207043307\n",
      "Step: 40900 loss: 0.005317857816803695\n",
      "Step: 41000 loss: 0.003969680665877533\n",
      "Step: 41100 loss: 0.0017498527249424\n",
      "Step: 41200 loss: 0.001766437607283251\n",
      "Step: 41300 loss: 0.006390773556929617\n",
      "Step: 41400 loss: 0.006699025587159326\n",
      "Step: 41500 loss: 0.003108181473258469\n",
      "Step: 41600 loss: 0.0035066806437546918\n",
      "Step: 41700 loss: 0.0028532561567249106\n",
      "Step: 41800 loss: 0.0023432152151622175\n",
      "Step: 41900 loss: 0.003170239647420914\n",
      "Step: 42000 loss: 0.004728953895468066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 42100 loss: 0.002025233378104332\n",
      "Step: 42200 loss: 0.001971899538546893\n",
      "Step: 42300 loss: 0.0009507109739570296\n",
      "Step: 42400 loss: 0.0019003577536736316\n",
      "Step: 42500 loss: 0.007341923468500369\n",
      "Step: 42600 loss: 0.003841564080692308\n",
      "Step: 42700 loss: 0.0010812711903294314\n",
      "Step: 42800 loss: 0.002277383367111554\n",
      "Step: 42900 loss: 0.005512177790427586\n",
      "Step: 43000 loss: 0.001761825229613976\n",
      "Step: 43100 loss: 0.0019925987418157832\n",
      "Step: 43200 loss: 0.004688686827982451\n",
      "Step: 43300 loss: 0.0043935747621520705\n",
      "Step: 43400 loss: 0.000564281762143537\n",
      "Step: 43500 loss: 0.002644684177989802\n",
      "Step: 43600 loss: 0.003434341382094459\n",
      "Step: 43700 loss: 0.0033344997667310848\n",
      "Step: 43800 loss: 0.0020147325061327594\n",
      "Step: 43900 loss: 0.0031451268388616427\n",
      "Step: 44000 loss: 0.0018334619241092299\n",
      "Step: 44100 loss: 0.003557969974601178\n",
      "Step: 44200 loss: 0.0013908005270622682\n",
      "Step: 44300 loss: 0.002705372204034404\n",
      "Step: 44400 loss: 0.00162263286229134\n",
      "Step: 44500 loss: 0.004596065963088876\n",
      "Step: 44600 loss: 0.0010809795012776391\n",
      "Step: 44700 loss: 0.00030139599044801455\n",
      "Step: 44800 loss: 0.0009240395090863273\n",
      "Step: 44900 loss: 0.0030163983833259066\n",
      "Step: 45000 loss: 0.0033763505592852992\n",
      "Step: 45100 loss: 0.0010365273146817344\n",
      "Step: 45200 loss: 0.0029889245573122025\n",
      "Step: 45300 loss: 0.0004868857245180891\n",
      "Step: 45400 loss: 0.002017435511357064\n",
      "Step: 45500 loss: 0.0011291796325706117\n",
      "Step: 45600 loss: 0.0039058090443597846\n",
      "Step: 45700 loss: 0.003955127715314148\n",
      "Step: 45800 loss: 0.002420665314540713\n",
      "Step: 45900 loss: 0.004191399728309335\n",
      "Step: 46000 loss: 0.0025532275816618722\n",
      "Step: 46100 loss: 0.0028734177196076873\n",
      "Step: 46200 loss: 0.0018384622337680413\n",
      "Step: 46300 loss: 0.008318351687900644\n",
      "Step: 46400 loss: 0.005211239572280135\n",
      "Step: 46500 loss: 0.005505299711238792\n",
      "Step: 46600 loss: 0.0004481073624000942\n",
      "Step: 46700 loss: 0.002246524472460436\n",
      "Step: 46800 loss: 0.00422675029237638\n",
      "Step: 46900 loss: 0.0021176120534167355\n",
      "Step: 47000 loss: 0.0009375858016692006\n",
      "Step: 47100 loss: 0.003753738011319143\n",
      "Step: 47200 loss: 0.0018527179582048347\n",
      "Step: 47300 loss: 0.008434883964424955\n",
      "Step: 47400 loss: 0.0019721868507467377\n",
      "Step: 47500 loss: 0.004962843305968931\n",
      "Step: 47600 loss: 0.004912034994016068\n",
      "Step: 47700 loss: 0.0028815269561602006\n",
      "Step: 47800 loss: 0.0037122362496032225\n",
      "Step: 47900 loss: 0.0014783225700873714\n",
      "Step: 48000 loss: 0.0053014495456962865\n",
      "Step: 48100 loss: 0.0031129888495260616\n",
      "Step: 48200 loss: 0.0009916733885656725\n",
      "Step: 48300 loss: 0.0030687949075436194\n",
      "Step: 48400 loss: 0.002692087942104706\n",
      "Step: 48500 loss: 0.0004106634430404199\n",
      "Step: 48600 loss: 0.0009371997834162471\n",
      "Step: 48700 loss: 0.0021197146587874725\n",
      "Step: 48800 loss: 0.0002688884137636194\n",
      "Step: 48900 loss: 0.0005691524759107835\n",
      "Step: 49000 loss: 0.005561247728567764\n",
      "Step: 49100 loss: 0.0075960513938672135\n",
      "Step: 49200 loss: 0.002093241396171663\n",
      "Step: 49300 loss: 0.002400461282954609\n",
      "Step: 49400 loss: 0.0004841269747346644\n",
      "Step: 49500 loss: 0.0028369967584483645\n",
      "Step: 49600 loss: 0.0065536547598826185\n",
      "Step: 49700 loss: 0.0034235927638187036\n",
      "Step: 49800 loss: 0.0026862669404015094\n",
      "Step: 49900 loss: 0.004980638113964204\n",
      "Step: 50000 loss: 0.0020827987557882465\n",
      "Step: 50100 loss: 0.00015306668712327108\n",
      "Step: 50200 loss: 0.0012851776837737817\n",
      "Step: 50300 loss: 0.0041755720533708995\n",
      "Step: 50400 loss: 0.007386463548601512\n",
      "Step: 50500 loss: 0.006018705014300281\n",
      "Step: 50600 loss: 0.0036430999119991727\n",
      "Step: 50700 loss: 0.0020682191581292385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7817cf3b7980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Step: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' loss: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "ai_conv_kernel = 128\n",
    "ai = AI(ai_conv_kernel)\n",
    "\n",
    "max_mem = 1000\n",
    "a = 28\n",
    "\n",
    "inp = []\n",
    "for i in range(max_mem):\n",
    "    inp.append(np.random.randn(1, a, 28))\n",
    "    \n",
    "criterion = nn.MSELoss()#SmoothL1Loss()MSELoss\n",
    "optimizer = optim.RMSprop(ai.parameters(), lr=0.00005)\n",
    "\n",
    "losses = 0\n",
    "for j in range(1):\n",
    "    for i in range(60000):#train_images.shape[0]):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            inp1 = train_images[i]#inp[i % max_mem].copy() + np.random.randn(1, a, 28) * 0.1\n",
    "            inp1.shape = (1,1,28,28)\n",
    "            inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "            out = ai(inp1)\n",
    "    #        print(out)\n",
    "    #        print(target)\n",
    "#            target = out.cpu().detach().numpy().copy()\n",
    "#            print(target.shape)\n",
    "#            target[0][8 - 1] = np.zeros(10)\n",
    "#            target[0][8 - 1][train_labels[i]] = 1.0\n",
    "\n",
    "#            target[0] = np.zeros([1,8,10])\n",
    "            target = np.zeros(10)\n",
    "            target[train_labels[i]] = 1.0\n",
    "#            for o in range(ai_conv_kernel):\n",
    "#                oo = (o + 1) / ai_conv_kernel\n",
    "#                oo = oo * oo\n",
    "#                target[0][o] = oo * n + (1.0 - oo) * target[0][o]\n",
    "#            target[0][8 - 1][train_labels[i]] = 1.0\n",
    "#            print(target)\n",
    "            target = T.from_numpy(np.array(target)).to(ai.device)\n",
    "            loss = criterion(out, target.float())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        losses = losses + loss.item()\n",
    "        if (i % 100) == 0:\n",
    "            print('Step: ' + str(i) + ' loss: ' + str(losses / 100))\n",
    "            losses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(10000):#train_images.shape[0]):\n",
    "    optimizer.zero_grad()\n",
    "    inp1 = test_images[i]#inp[i % max_mem].copy()\n",
    "    inp1.shape = (1,1,28,28)\n",
    "    inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "    out = ai(inp1).cpu().detach().numpy()\n",
    "    m = np.argmax(out)\n",
    "    if m == test_labels[i]:\n",
    "#        print(m)\n",
    "        count = count + 1\n",
    "print(\"correct: \" + str(count / 10000))#train_images.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
