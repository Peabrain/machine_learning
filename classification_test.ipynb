{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI(nn.Module):\n",
    "    def __init__(self,conv_kernel):\n",
    "        super(AI, self).__init__()\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.lstm_layers = 1\n",
    "        self.lstm_depth = 1000\n",
    "        self.cnn1 = nn.Conv2d(1,self.conv_kernel,5,stride=2,padding=1)\n",
    "        self.cnn2 = nn.Conv2d(self.conv_kernel,self.conv_kernel,3,stride=2,padding=0)\n",
    "        self.rnn = nn.LSTM(6 * 6, self.lstm_depth,self.lstm_layers)\n",
    "        self.fc1 = nn.Linear(self.lstm_depth,self.lstm_depth)\n",
    "        self.fc2 = nn.Linear(self.lstm_depth,10)\n",
    "        self.out = nn.Dropout(0.9)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')     \n",
    "        self.to(self.device)\n",
    "    def forward(self,x):\n",
    "        hx = T.zeros(self.lstm_layers,self.conv_kernel, self.lstm_depth).to(self.device)\n",
    "        cx = T.zeros(self.lstm_layers,self.conv_kernel, self.lstm_depth).to(self.device)\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "#        print(x.shape)\n",
    "        x = x.view(-1,self.conv_kernel,6 * 6)\n",
    "        hx, cx = self.rnn(x, (hx, cx))\n",
    "        o = T.tanh(self.fc1(hx))\n",
    "        o = T.sigmoid(self.fc2(o))\n",
    "#        o = self.out(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLAYFIELD():\n",
    "    def __init__(self,width,height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pf = np.zeros((self.width,self.height,2))\n",
    "    def setStone(self,x,y,stone):\n",
    "        s = np.zeros(2)\n",
    "        s[stone] = 1\n",
    "        self.pf[y][x] = s\n",
    "    def get(self):\n",
    "        return self.pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:2: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:4: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "with open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "with open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_labels = extract_labels(f)\n",
    "\n",
    "with open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "with open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = extract_labels(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 loss: 0.0005046569555997849\n",
      "Step: 100 loss: 0.01817169249523431\n",
      "Step: 200 loss: 0.012863098946399986\n",
      "Step: 300 loss: 0.009757395560154692\n",
      "Step: 400 loss: 0.007608264443697408\n",
      "Step: 500 loss: 0.007961103776469827\n",
      "Step: 600 loss: 0.00827303237048909\n",
      "Step: 700 loss: 0.007086562535841949\n",
      "Step: 800 loss: 0.006514218118391\n",
      "Step: 900 loss: 0.006338046362507157\n",
      "Step: 1000 loss: 0.005819015542947455\n",
      "Step: 1100 loss: 0.006867442991642747\n",
      "Step: 1200 loss: 0.005731767673860304\n",
      "Step: 1300 loss: 0.006285862107179127\n",
      "Step: 1400 loss: 0.004916772861324716\n",
      "Step: 1500 loss: 0.004321016228204826\n",
      "Step: 1600 loss: 0.004474301819282118\n",
      "Step: 1700 loss: 0.003612013930542162\n",
      "Step: 1800 loss: 0.0028671061703062152\n",
      "Step: 1900 loss: 0.003611241973412689\n",
      "Step: 2000 loss: 0.0041458495514962125\n",
      "Step: 2100 loss: 0.004493254055851139\n",
      "Step: 2200 loss: 0.002982811086840229\n",
      "Step: 2300 loss: 0.003472946363981464\n",
      "Step: 2400 loss: 0.0033579270273185104\n",
      "Step: 2500 loss: 0.0034080125753825994\n",
      "Step: 2600 loss: 0.0029160752258758295\n",
      "Step: 2700 loss: 0.003608383696555393\n",
      "Step: 2800 loss: 0.004063555704924511\n",
      "Step: 2900 loss: 0.0031243317413463955\n",
      "Step: 3000 loss: 0.003580487392573559\n",
      "Step: 3100 loss: 0.003628512761933962\n",
      "Step: 3200 loss: 0.0022309456754737765\n",
      "Step: 3300 loss: 0.003251976268584258\n",
      "Step: 3400 loss: 0.00250941958893236\n",
      "Step: 3500 loss: 0.002177724504326761\n",
      "Step: 3600 loss: 0.002987631828036683\n",
      "Step: 3700 loss: 0.003087264857495029\n",
      "Step: 3800 loss: 0.0032018269841137224\n",
      "Step: 3900 loss: 0.00259521839019726\n",
      "Step: 4000 loss: 0.0024217764554487074\n",
      "Step: 4100 loss: 0.0029173901737522103\n",
      "Step: 4200 loss: 0.003612606333117583\n",
      "Step: 4300 loss: 0.0023879542748454696\n",
      "Step: 4400 loss: 0.003587767160770454\n",
      "Step: 4500 loss: 0.00273944542854224\n",
      "Step: 4600 loss: 0.002367904479151548\n",
      "Step: 4700 loss: 0.0033133204753357857\n",
      "Step: 4800 loss: 0.0030231262617598986\n",
      "Step: 4900 loss: 0.002068538921776053\n",
      "Step: 5000 loss: 0.0030665924431832535\n",
      "Step: 5100 loss: 0.0025036051294136997\n",
      "Step: 5200 loss: 0.0040291590833112425\n",
      "Step: 5300 loss: 0.001993113705830183\n",
      "Step: 5400 loss: 0.002844668423381336\n",
      "Step: 5500 loss: 0.002043602896883385\n",
      "Step: 5600 loss: 0.0031288769936418247\n",
      "Step: 5700 loss: 0.002555710845081194\n",
      "Step: 5800 loss: 0.002990343154497168\n",
      "Step: 5900 loss: 0.002779684720271689\n",
      "Step: 6000 loss: 0.001877913312068813\n",
      "Step: 6100 loss: 0.0016512841431813286\n",
      "Step: 6200 loss: 0.0012060354965342413\n",
      "Step: 6300 loss: 0.0026524765738940915\n",
      "Step: 6400 loss: 0.001338062784543581\n",
      "Step: 6500 loss: 0.002887098083883757\n",
      "Step: 6600 loss: 0.0019714940990616017\n",
      "Step: 6700 loss: 0.00243808712632017\n",
      "Step: 6800 loss: 0.001901030594385702\n",
      "Step: 6900 loss: 0.004422970949708543\n",
      "Step: 7000 loss: 0.0031435200059559067\n",
      "Step: 7100 loss: 0.003295820723296856\n",
      "Step: 7200 loss: 0.0028656270701321773\n",
      "Step: 7300 loss: 0.003554251310815744\n",
      "Step: 7400 loss: 0.0025307330376335814\n",
      "Step: 7500 loss: 0.0025845815585034872\n",
      "Step: 7600 loss: 0.0024523295776134545\n",
      "Step: 7700 loss: 0.0022884780639606107\n",
      "Step: 7800 loss: 0.002863405407197206\n",
      "Step: 7900 loss: 0.002892994256735619\n",
      "Step: 8000 loss: 0.002593092271254136\n",
      "Step: 8100 loss: 0.0011241867383569115\n",
      "Step: 8200 loss: 0.0027280375821146664\n",
      "Step: 8300 loss: 0.0036812952180434876\n",
      "Step: 8400 loss: 0.0015130488104341565\n",
      "Step: 8500 loss: 0.0026064309581579435\n",
      "Step: 8600 loss: 0.002743969003754501\n",
      "Step: 8700 loss: 0.003637790814132131\n",
      "Step: 8800 loss: 0.004989723143685296\n",
      "Step: 8900 loss: 0.0032933565380972142\n",
      "Step: 9000 loss: 0.002436705656932645\n",
      "Step: 9100 loss: 0.0014624120382086402\n",
      "Step: 9200 loss: 0.0024075204922348803\n",
      "Step: 9300 loss: 0.0022709869095342583\n",
      "Step: 9400 loss: 0.002603782730907369\n",
      "Step: 9500 loss: 0.0022048926540037426\n",
      "Step: 9600 loss: 0.0030253717860546205\n",
      "Step: 9700 loss: 0.0017256979135754592\n",
      "Step: 9800 loss: 0.0018055540180944264\n",
      "Step: 9900 loss: 0.00161162675825949\n",
      "Step: 10000 loss: 0.0014502852808323042\n",
      "Step: 10100 loss: 0.0022515831300415814\n",
      "Step: 10200 loss: 0.0028490160162982646\n",
      "Step: 10300 loss: 0.003002059415525764\n",
      "Step: 10400 loss: 0.0016192931099817543\n",
      "Step: 10500 loss: 0.0012093096518538005\n",
      "Step: 10600 loss: 0.0010765856751982028\n",
      "Step: 10700 loss: 0.0012425929114533573\n",
      "Step: 10800 loss: 0.00248777897865466\n",
      "Step: 10900 loss: 0.001305544178328546\n",
      "Step: 11000 loss: 0.0014328240016038762\n",
      "Step: 11100 loss: 0.001968202683567597\n",
      "Step: 11200 loss: 0.0017301875782777642\n",
      "Step: 11300 loss: 0.0023575009344313004\n",
      "Step: 11400 loss: 0.0017442786704896208\n",
      "Step: 11500 loss: 0.0023753106940853285\n",
      "Step: 11600 loss: 0.003159904991713347\n",
      "Step: 11700 loss: 0.002119107267335494\n",
      "Step: 11800 loss: 0.0026587811952958874\n",
      "Step: 11900 loss: 0.002299405446228775\n",
      "Step: 12000 loss: 0.0015200646984499144\n",
      "Step: 12100 loss: 0.0016641781598809757\n",
      "Step: 12200 loss: 0.0011056620095632752\n",
      "Step: 12300 loss: 0.0026187101358812015\n",
      "Step: 12400 loss: 0.002431779777673455\n",
      "Step: 12500 loss: 0.0025219445139418894\n",
      "Step: 12600 loss: 0.002664372665752808\n",
      "Step: 12700 loss: 0.0036142640528100856\n",
      "Step: 12800 loss: 0.0024556869525508775\n",
      "Step: 12900 loss: 0.0025085102562275095\n",
      "Step: 13000 loss: 0.0028095574991994\n",
      "Step: 13100 loss: 0.004385787098835863\n",
      "Step: 13200 loss: 0.0027270581346101608\n",
      "Step: 13300 loss: 0.0008320190671645377\n",
      "Step: 13400 loss: 0.002488781155363995\n",
      "Step: 13500 loss: 0.001960394719512806\n",
      "Step: 13600 loss: 0.0014657415590011169\n",
      "Step: 13700 loss: 0.0021301965675695556\n",
      "Step: 13800 loss: 0.0027145206021791067\n",
      "Step: 13900 loss: 0.0019425635620950743\n",
      "Step: 14000 loss: 0.002888704769848118\n",
      "Step: 14100 loss: 0.0022124978957083386\n",
      "Step: 14200 loss: 0.002463767401043242\n",
      "Step: 14300 loss: 0.0028247018092656616\n",
      "Step: 14400 loss: 0.0036952255741039153\n",
      "Step: 14500 loss: 0.001543603093498973\n",
      "Step: 14600 loss: 0.0019280180768839728\n",
      "Step: 14700 loss: 0.003119040061629903\n",
      "Step: 14800 loss: 0.0038136404355986996\n",
      "Step: 14900 loss: 0.001632409370129153\n",
      "Step: 15000 loss: 0.0016646587711943539\n",
      "Step: 15100 loss: 0.0012361321867024344\n",
      "Step: 15200 loss: 0.0033280185528201402\n",
      "Step: 15300 loss: 0.001244235839054113\n",
      "Step: 15400 loss: 0.0018566234659954262\n",
      "Step: 15500 loss: 0.0020005335975054094\n",
      "Step: 15600 loss: 0.001512873680200073\n",
      "Step: 15700 loss: 0.0013104582244773154\n",
      "Step: 15800 loss: 0.0038516269749743516\n",
      "Step: 15900 loss: 0.0031415587463584417\n",
      "Step: 16000 loss: 0.0018414249333147837\n",
      "Step: 16100 loss: 0.002871112300711047\n",
      "Step: 16200 loss: 0.001957448265670223\n",
      "Step: 16300 loss: 0.0011603763968219027\n",
      "Step: 16400 loss: 0.0013543296775014825\n",
      "Step: 16500 loss: 0.0019358808553249674\n",
      "Step: 16600 loss: 0.0024305465147222093\n",
      "Step: 16700 loss: 0.00263123445798783\n",
      "Step: 16800 loss: 0.0027295395479188756\n",
      "Step: 16900 loss: 0.001809207034596909\n",
      "Step: 17000 loss: 0.002183193672692596\n",
      "Step: 17100 loss: 0.002237768863569727\n",
      "Step: 17200 loss: 0.0025917668752745726\n",
      "Step: 17300 loss: 0.0023492558139696484\n",
      "Step: 17400 loss: 0.0012409851573607967\n",
      "Step: 17500 loss: 0.0018953756236692243\n",
      "Step: 17600 loss: 0.00225702229753324\n",
      "Step: 17700 loss: 0.0019942714773674196\n",
      "Step: 17800 loss: 0.002901033147859664\n",
      "Step: 17900 loss: 0.003009155099379086\n",
      "Step: 18000 loss: 0.00119981634418167\n",
      "Step: 18100 loss: 0.001822089979683028\n",
      "Step: 18200 loss: 0.0011221728156570522\n",
      "Step: 18300 loss: 0.0013918554451100818\n",
      "Step: 18400 loss: 0.0012551404450084646\n",
      "Step: 18500 loss: 0.0016663361044541602\n",
      "Step: 18600 loss: 0.001338220333936988\n",
      "Step: 18700 loss: 0.0015801408055898492\n",
      "Step: 18800 loss: 0.0018797553494789554\n",
      "Step: 18900 loss: 0.001227697749153549\n",
      "Step: 19000 loss: 0.0007002753059549605\n",
      "Step: 19100 loss: 0.0019912293568930296\n",
      "Step: 19200 loss: 0.002020692850354635\n",
      "Step: 19300 loss: 0.0027130042388654376\n",
      "Step: 19400 loss: 0.002099043151929436\n",
      "Step: 19500 loss: 0.0011979707341896529\n",
      "Step: 19600 loss: 0.0020661425186665385\n",
      "Step: 19700 loss: 0.001069123089825439\n",
      "Step: 19800 loss: 0.0011079526292886045\n",
      "Step: 19900 loss: 0.002331550608708994\n",
      "Step: 20000 loss: 0.0012381140187574147\n",
      "Step: 20100 loss: 0.0022270808851871495\n",
      "Step: 20200 loss: 0.0024433327819258464\n",
      "Step: 20300 loss: 0.002097246751785633\n",
      "Step: 20400 loss: 0.0011713411319360034\n",
      "Step: 20500 loss: 0.0014472619188939007\n",
      "Step: 20600 loss: 0.0016831625879493118\n",
      "Step: 20700 loss: 0.001980531775271004\n",
      "Step: 20800 loss: 0.0034476549556704582\n",
      "Step: 20900 loss: 0.0019718137162229254\n",
      "Step: 21000 loss: 0.0032837620948316724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21100 loss: 0.0018355934805939001\n",
      "Step: 21200 loss: 0.0015983436374904158\n",
      "Step: 21300 loss: 0.002020712752192253\n",
      "Step: 21400 loss: 0.002117808804443655\n",
      "Step: 21500 loss: 0.0017531432498208233\n",
      "Step: 21600 loss: 0.0016013273025333773\n",
      "Step: 21700 loss: 0.0015350043096643163\n",
      "Step: 21800 loss: 0.0015093921632222873\n",
      "Step: 21900 loss: 0.0009767528822999339\n",
      "Step: 22000 loss: 0.0021328365136309914\n",
      "Step: 22100 loss: 0.0011789420455821187\n",
      "Step: 22200 loss: 0.0029671033096170165\n",
      "Step: 22300 loss: 0.00268715617099474\n",
      "Step: 22400 loss: 0.0014626959532762385\n",
      "Step: 22500 loss: 0.003239534650428766\n",
      "Step: 22600 loss: 0.0036299828219563323\n",
      "Step: 22700 loss: 0.0023891386762159073\n",
      "Step: 22800 loss: 0.0019422320698873818\n",
      "Step: 22900 loss: 0.0021722476890488453\n",
      "Step: 23000 loss: 0.0002726817906741985\n",
      "Step: 23100 loss: 0.0018496547346299507\n",
      "Step: 23200 loss: 0.0015528671749699186\n",
      "Step: 23300 loss: 0.0018695671261949086\n",
      "Step: 23400 loss: 0.0017287999813329691\n",
      "Step: 23500 loss: 0.0017825345905009727\n",
      "Step: 23600 loss: 0.001743304537486523\n",
      "Step: 23700 loss: 0.0021702106956851706\n",
      "Step: 23800 loss: 0.0023708370624638065\n",
      "Step: 23900 loss: 0.0013850697857998285\n",
      "Step: 24000 loss: 0.0016401765444802408\n",
      "Step: 24100 loss: 0.0018563885270224034\n",
      "Step: 24200 loss: 0.0016583492636925712\n",
      "Step: 24300 loss: 0.002811879280618683\n",
      "Step: 24400 loss: 0.0004565691963372842\n",
      "Step: 24500 loss: 0.001344426386344111\n",
      "Step: 24600 loss: 0.0015521262682506177\n",
      "Step: 24700 loss: 0.0028275282734614394\n",
      "Step: 24800 loss: 0.002154597426594478\n",
      "Step: 24900 loss: 0.0017830039928668385\n",
      "Step: 25000 loss: 0.001674642820316592\n",
      "Step: 25100 loss: 0.0017415007994858911\n",
      "Step: 25200 loss: 0.001649430032751411\n",
      "Step: 25300 loss: 0.001991765909487242\n",
      "Step: 25400 loss: 0.0011311434885209338\n",
      "Step: 25500 loss: 0.0015986246875124266\n",
      "Step: 25600 loss: 0.0017688341782330142\n",
      "Step: 25700 loss: 0.0016244775827746593\n",
      "Step: 25800 loss: 0.0014951752095399228\n",
      "Step: 25900 loss: 0.0021562521204094767\n",
      "Step: 26000 loss: 0.0023336335315452318\n",
      "Step: 26100 loss: 0.001330068725905278\n",
      "Step: 26200 loss: 0.0011385204165379292\n",
      "Step: 26300 loss: 0.0014376133785059864\n",
      "Step: 26400 loss: 0.0024186034374089616\n",
      "Step: 26500 loss: 0.0018588410580770898\n",
      "Step: 26600 loss: 0.0021206611870915994\n",
      "Step: 26700 loss: 0.0025284622206187633\n",
      "Step: 26800 loss: 0.0034306050414721766\n",
      "Step: 26900 loss: 0.0026498259896928287\n",
      "Step: 27000 loss: 0.0009196379708299674\n",
      "Step: 27100 loss: 0.0018207420729336832\n",
      "Step: 27200 loss: 0.002991456995774513\n",
      "Step: 27300 loss: 0.0020947762266854395\n",
      "Step: 27400 loss: 0.0011229483270916774\n",
      "Step: 27500 loss: 0.0019870896134199256\n",
      "Step: 27600 loss: 0.002510554538354768\n",
      "Step: 27700 loss: 0.0018376177461038167\n",
      "Step: 27800 loss: 0.0018365084174996583\n",
      "Step: 27900 loss: 0.0009944369643653773\n",
      "Step: 28000 loss: 0.0014739091589851937\n",
      "Step: 28100 loss: 0.0011976951330330365\n",
      "Step: 28200 loss: 0.0023948146347902368\n",
      "Step: 28300 loss: 0.0009717048300971953\n",
      "Step: 28400 loss: 0.002117566721269064\n",
      "Step: 28500 loss: 0.0010891152343999978\n",
      "Step: 28600 loss: 0.0018193970509643976\n",
      "Step: 28700 loss: 0.002677281302585044\n",
      "Step: 28800 loss: 0.0019474125286600952\n",
      "Step: 28900 loss: 0.0012838366051346384\n",
      "Step: 29000 loss: 0.0014961084858123286\n",
      "Step: 29100 loss: 0.0022883832412053805\n",
      "Step: 29200 loss: 0.0029467371190207816\n",
      "Step: 29300 loss: 0.002527802534405055\n",
      "Step: 29400 loss: 0.0031042603099425037\n",
      "Step: 29500 loss: 0.0014632469645546565\n",
      "Step: 29600 loss: 0.0016092720904316593\n",
      "Step: 29700 loss: 0.0011950426297336492\n",
      "Step: 29800 loss: 0.0014670993058800263\n",
      "Step: 29900 loss: 0.0020709578465275057\n",
      "Step: 30000 loss: 0.002679390087541549\n",
      "Step: 30100 loss: 0.0018805467241037378\n",
      "Step: 30200 loss: 0.0022454550517078077\n",
      "Step: 30300 loss: 0.001403471246426875\n",
      "Step: 30400 loss: 0.001491145152894404\n",
      "Step: 30500 loss: 0.001947197372279561\n",
      "Step: 30600 loss: 0.0014736706841051727\n",
      "Step: 30700 loss: 0.0025492810712076162\n",
      "Step: 30800 loss: 0.0020902414909083687\n",
      "Step: 30900 loss: 0.0021853361444131637\n",
      "Step: 31000 loss: 0.001516872376837739\n",
      "Step: 31100 loss: 0.0011063550605966554\n",
      "Step: 31200 loss: 0.0024395166233207987\n",
      "Step: 31300 loss: 0.001896732972947035\n",
      "Step: 31400 loss: 0.0033475876257517\n",
      "Step: 31500 loss: 0.0021571658592311137\n",
      "Step: 31600 loss: 0.0017642015714667992\n",
      "Step: 31700 loss: 0.002287878271599766\n",
      "Step: 31800 loss: 0.0030369014758566946\n",
      "Step: 31900 loss: 0.0013760763670538267\n",
      "Step: 32000 loss: 0.001464674230483638\n",
      "Step: 32100 loss: 0.002548606109681941\n",
      "Step: 32200 loss: 0.0021584837366052058\n",
      "Step: 32300 loss: 0.0015230312403577528\n",
      "Step: 32400 loss: 0.002011077119081506\n",
      "Step: 32500 loss: 0.0024627621984073754\n",
      "Step: 32600 loss: 0.0008343913053668218\n",
      "Step: 32700 loss: 0.001526775769202473\n",
      "Step: 32800 loss: 0.001345574230338684\n",
      "Step: 32900 loss: 0.0017461261953678785\n",
      "Step: 33000 loss: 0.001070605306953567\n",
      "Step: 33100 loss: 0.000748251377656004\n",
      "Step: 33200 loss: 0.0009581851862350277\n",
      "Step: 33300 loss: 0.0008841237141845682\n",
      "Step: 33400 loss: 0.0024155167602643245\n",
      "Step: 33500 loss: 0.0011788468659305806\n",
      "Step: 33600 loss: 0.0011433162782987338\n",
      "Step: 33700 loss: 0.0012102138927972562\n",
      "Step: 33800 loss: 0.0020233721461061904\n",
      "Step: 33900 loss: 0.0013870529010195831\n",
      "Step: 34000 loss: 0.0005150680680447905\n",
      "Step: 34100 loss: 0.0020696689620007193\n",
      "Step: 34200 loss: 0.0006703577612557865\n",
      "Step: 34300 loss: 0.00041524357197900483\n",
      "Step: 34400 loss: 0.0014805051544473714\n",
      "Step: 34500 loss: 0.002502581285055925\n",
      "Step: 34600 loss: 0.0017725355562645717\n",
      "Step: 34700 loss: 0.002268226635329995\n",
      "Step: 34800 loss: 0.0024597738747765873\n",
      "Step: 34900 loss: 0.002001165806022698\n",
      "Step: 35000 loss: 0.001876136581641088\n",
      "Step: 35100 loss: 0.0010723643073757216\n",
      "Step: 35200 loss: 0.0016103659058953922\n",
      "Step: 35300 loss: 0.0015582136538430903\n",
      "Step: 35400 loss: 0.0016369122482308284\n",
      "Step: 35500 loss: 0.0016584842694004464\n",
      "Step: 35600 loss: 0.0012224445785265915\n",
      "Step: 35700 loss: 0.0026662493239186704\n",
      "Step: 35800 loss: 0.0011518684366494726\n",
      "Step: 35900 loss: 0.0009286560541133681\n",
      "Step: 36000 loss: 0.001349810094925008\n",
      "Step: 36100 loss: 0.0017990544017409604\n",
      "Step: 36200 loss: 0.0018704482921703658\n",
      "Step: 36300 loss: 0.0016237166010872528\n",
      "Step: 36400 loss: 0.00099816020285715\n",
      "Step: 36500 loss: 0.001922394888535308\n",
      "Step: 36600 loss: 0.001362937892249505\n",
      "Step: 36700 loss: 0.0004720965924296294\n",
      "Step: 36800 loss: 0.0015402232034081465\n",
      "Step: 36900 loss: 0.002532843638523019\n",
      "Step: 37000 loss: 0.0011675688692209718\n",
      "Step: 37100 loss: 0.002487727792161323\n",
      "Step: 37200 loss: 0.001996540661705737\n",
      "Step: 37300 loss: 0.0013226986745035418\n",
      "Step: 37400 loss: 0.0020971589242668644\n",
      "Step: 37500 loss: 0.0021837961630144774\n",
      "Step: 37600 loss: 0.001842865006371426\n",
      "Step: 37700 loss: 0.0017028916734913935\n",
      "Step: 37800 loss: 0.0017027993308280998\n",
      "Step: 37900 loss: 0.0017305490381220778\n",
      "Step: 38000 loss: 0.00150527150314133\n",
      "Step: 38100 loss: 0.001500207309253092\n",
      "Step: 38200 loss: 0.0008927307914039417\n",
      "Step: 38300 loss: 0.0017303513024121298\n",
      "Step: 38400 loss: 0.002610925926809653\n",
      "Step: 38500 loss: 0.0009039949188605334\n",
      "Step: 38600 loss: 0.0025137401733100616\n",
      "Step: 38700 loss: 0.0018219421113776946\n",
      "Step: 38800 loss: 0.0023156650878905084\n",
      "Step: 38900 loss: 0.0006107031341074887\n",
      "Step: 39000 loss: 0.0009233014911988846\n",
      "Step: 39100 loss: 0.000440169142492266\n",
      "Step: 39200 loss: 0.0011435651057158402\n",
      "Step: 39300 loss: 0.0016047735630097737\n",
      "Step: 39400 loss: 0.003448587890175361\n",
      "Step: 39500 loss: 0.0018407108361529012\n",
      "Step: 39600 loss: 0.0012898498500319988\n",
      "Step: 39700 loss: 0.00147682773593516\n",
      "Step: 39800 loss: 0.0015549425408342898\n",
      "Step: 39900 loss: 0.002217975110280648\n",
      "Step: 40000 loss: 0.0020971225586503195\n",
      "Step: 40100 loss: 0.0014955234773334424\n",
      "Step: 40200 loss: 0.001105398855623676\n",
      "Step: 40300 loss: 0.0012649138533294036\n",
      "Step: 40400 loss: 0.0018959688539100837\n",
      "Step: 40500 loss: 0.001005431787972526\n",
      "Step: 40600 loss: 0.001237796967500895\n",
      "Step: 40700 loss: 0.00230444860699869\n",
      "Step: 40800 loss: 0.001413549803360965\n",
      "Step: 40900 loss: 0.001971199954378271\n",
      "Step: 41000 loss: 0.0009228387450865583\n",
      "Step: 41100 loss: 0.0020478800002140486\n",
      "Step: 41200 loss: 0.0006463025300534753\n",
      "Step: 41300 loss: 0.0023605045616288223\n",
      "Step: 41400 loss: 0.0022405991799654855\n",
      "Step: 41500 loss: 0.00244622215715971\n",
      "Step: 41600 loss: 0.0020672485502551295\n",
      "Step: 41700 loss: 0.0008541460515036547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41800 loss: 0.001050344620508028\n",
      "Step: 41900 loss: 0.0008869766763665155\n",
      "Step: 42000 loss: 0.0021347875613574275\n",
      "Step: 42100 loss: 0.0014563210960925944\n",
      "Step: 42200 loss: 0.0016212739999935622\n",
      "Step: 42300 loss: 0.0006336527117355217\n",
      "Step: 42400 loss: 0.0018524944721953318\n",
      "Step: 42500 loss: 0.0017946608329594404\n",
      "Step: 42600 loss: 0.0017285649855168827\n",
      "Step: 42700 loss: 0.0021286509376960082\n",
      "Step: 42800 loss: 0.0015023566651799314\n",
      "Step: 42900 loss: 0.002077143454157735\n",
      "Step: 43000 loss: 0.002937463220169789\n",
      "Step: 43100 loss: 0.001862028916157179\n",
      "Step: 43200 loss: 0.0015413248099489963\n",
      "Step: 43300 loss: 0.001576594021845743\n",
      "Step: 43400 loss: 0.00016940640480353153\n",
      "Step: 43500 loss: 0.001250372108179718\n",
      "Step: 43600 loss: 0.0019007570957739262\n",
      "Step: 43700 loss: 0.0015516138296054027\n",
      "Step: 43800 loss: 0.0007665712650778111\n",
      "Step: 43900 loss: 0.0009085928822441503\n",
      "Step: 44000 loss: 0.0014063388162635527\n",
      "Step: 44100 loss: 0.0011683278786968998\n",
      "Step: 44200 loss: 0.0014506484261334763\n",
      "Step: 44300 loss: 0.0023099426504310117\n",
      "Step: 44400 loss: 0.0013413029666556398\n",
      "Step: 44500 loss: 0.0020381919478208842\n",
      "Step: 44600 loss: 0.0005523316998796091\n",
      "Step: 44700 loss: 0.0011013552258888027\n",
      "Step: 44800 loss: 0.0009782634795775635\n",
      "Step: 44900 loss: 0.001601553177506112\n",
      "Step: 45000 loss: 0.002743904287210377\n",
      "Step: 45100 loss: 0.002900261395985666\n",
      "Step: 45200 loss: 0.0010735981399069682\n",
      "Step: 45300 loss: 0.0009603521946948046\n",
      "Step: 45400 loss: 0.0009013337122797527\n",
      "Step: 45500 loss: 0.0028675361653476726\n",
      "Step: 45600 loss: 0.0028044486703389993\n",
      "Step: 45700 loss: 0.0019879809380051582\n",
      "Step: 45800 loss: 0.0011958525822993948\n",
      "Step: 45900 loss: 0.0017438748967170525\n",
      "Step: 46000 loss: 0.0019305612933904648\n",
      "Step: 46100 loss: 0.002437485835745541\n",
      "Step: 46200 loss: 0.001820936797936401\n",
      "Step: 46300 loss: 0.002641208961531234\n",
      "Step: 46400 loss: 0.0031123826491636563\n",
      "Step: 46500 loss: 0.0020302729415092547\n",
      "Step: 46600 loss: 0.0011789307574452846\n",
      "Step: 46700 loss: 0.001205791183190499\n",
      "Step: 46800 loss: 0.0014760486726507337\n",
      "Step: 46900 loss: 0.0017195174542699654\n",
      "Step: 47000 loss: 0.001252431427078271\n",
      "Step: 47100 loss: 0.0016248911967858249\n",
      "Step: 47200 loss: 0.0006976359719977409\n",
      "Step: 47300 loss: 0.0027479312648497965\n",
      "Step: 47400 loss: 0.0019891404516763165\n",
      "Step: 47500 loss: 0.0025386188090719486\n",
      "Step: 47600 loss: 0.0019448353501259064\n",
      "Step: 47700 loss: 0.0014493524043607485\n",
      "Step: 47800 loss: 0.0010647872665878922\n",
      "Step: 47900 loss: 0.0010389742967765693\n",
      "Step: 48000 loss: 0.0018911358038459357\n",
      "Step: 48100 loss: 0.001701024715657571\n",
      "Step: 48200 loss: 0.001010349503634691\n",
      "Step: 48300 loss: 0.001306386787922733\n",
      "Step: 48400 loss: 0.001677331385051133\n",
      "Step: 48500 loss: 0.000455723290426997\n",
      "Step: 48600 loss: 0.0016810145463645831\n",
      "Step: 48700 loss: 0.0015832840320092068\n",
      "Step: 48800 loss: 0.000580359729451878\n",
      "Step: 48900 loss: 0.0011014112685269238\n",
      "Step: 49000 loss: 0.003064490613975197\n",
      "Step: 49100 loss: 0.0025269369594179557\n",
      "Step: 49200 loss: 0.0013370069611127633\n",
      "Step: 49300 loss: 0.0020593295330449024\n",
      "Step: 49400 loss: 0.0006564859960389935\n",
      "Step: 49500 loss: 0.00139195962846677\n",
      "Step: 49600 loss: 0.0028938444634060367\n",
      "Step: 49700 loss: 0.002583101954395932\n",
      "Step: 49800 loss: 0.0011120074002253677\n",
      "Step: 49900 loss: 0.001860091513879043\n",
      "Step: 50000 loss: 0.0008751264949229798\n",
      "Step: 50100 loss: 0.0011716692469404544\n",
      "Step: 50200 loss: 0.0008208385558567333\n",
      "Step: 50300 loss: 0.0025451245431129177\n",
      "Step: 50400 loss: 0.0035026023593772403\n",
      "Step: 50500 loss: 0.002721602577695137\n",
      "Step: 50600 loss: 0.0017811831253227229\n",
      "Step: 50700 loss: 0.001630535188911697\n",
      "Step: 50800 loss: 0.0013138536173583847\n",
      "Step: 50900 loss: 0.0009995481455597143\n",
      "Step: 51000 loss: 0.0004265187351408617\n",
      "Step: 51100 loss: 0.0010054957799850862\n",
      "Step: 51200 loss: 0.0012168464084312803\n",
      "Step: 51300 loss: 0.0023675673433400978\n",
      "Step: 51400 loss: 0.0018362177201282747\n",
      "Step: 51500 loss: 0.0010259142288999178\n",
      "Step: 51600 loss: 0.0010861146145822787\n",
      "Step: 51700 loss: 0.0014466111008072248\n",
      "Step: 51800 loss: 0.001199571276524054\n",
      "Step: 51900 loss: 0.0008919250825860558\n",
      "Step: 52000 loss: 0.0021747360818506413\n",
      "Step: 52100 loss: 0.0016846001345645423\n",
      "Step: 52200 loss: 0.0023006003197839587\n",
      "Step: 52300 loss: 0.0019680719515677934\n",
      "Step: 52400 loss: 0.001457458711548014\n",
      "Step: 52500 loss: 0.0008363725217183582\n",
      "Step: 52600 loss: 0.0010205176096492608\n",
      "Step: 52700 loss: 0.0015062475826693334\n",
      "Step: 52800 loss: 0.0022415252905158044\n",
      "Step: 52900 loss: 0.00209031003576958\n",
      "Step: 53000 loss: 0.0038670936175890258\n",
      "Step: 53100 loss: 0.0019432580917262765\n",
      "Step: 53200 loss: 0.001993442105796284\n",
      "Step: 53300 loss: 0.0006756047143509214\n",
      "Step: 53400 loss: 0.0006980699142620184\n",
      "Step: 53500 loss: 0.0011800633079881529\n",
      "Step: 53600 loss: 0.0021332848044148633\n",
      "Step: 53700 loss: 0.0024585140841162944\n",
      "Step: 53800 loss: 0.0009077695327175172\n",
      "Step: 53900 loss: 0.0018196228786010503\n",
      "Step: 54000 loss: 0.002086973600913229\n",
      "Step: 54100 loss: 0.0015804794529892163\n",
      "Step: 54200 loss: 0.0010522493061221638\n",
      "Step: 54300 loss: 0.0006729014311709314\n",
      "Step: 54400 loss: 0.0010410905772708777\n",
      "Step: 54500 loss: 0.0007284705209486563\n",
      "Step: 54600 loss: 0.001462850286002384\n",
      "Step: 54700 loss: 0.001068156682028385\n",
      "Step: 54800 loss: 0.0009040106991672125\n",
      "Step: 54900 loss: 0.0018117552792797297\n",
      "Step: 55000 loss: 0.002719412637452816\n",
      "Step: 55100 loss: 0.0012479320093900358\n",
      "Step: 55200 loss: 0.0013616140569389645\n",
      "Step: 55300 loss: 0.0015157489594204688\n",
      "Step: 55400 loss: 0.0019940025245012904\n",
      "Step: 55500 loss: 0.0015298711372607255\n",
      "Step: 55600 loss: 0.0008786606777394646\n",
      "Step: 55700 loss: 0.000821877993457254\n",
      "Step: 55800 loss: 0.0016203356939812163\n",
      "Step: 55900 loss: 0.00202264058247008\n",
      "Step: 56000 loss: 0.0009304763885036849\n",
      "Step: 56100 loss: 0.0016949142829728635\n",
      "Step: 56200 loss: 0.0014759886726186623\n",
      "Step: 56300 loss: 0.0022448435426451765\n",
      "Step: 56400 loss: 0.0012452211883423646\n",
      "Step: 56500 loss: 0.0023671181860332392\n",
      "Step: 56600 loss: 0.0014626024432854655\n",
      "Step: 56700 loss: 0.0012454224095108657\n",
      "Step: 56800 loss: 0.0014232745824784843\n",
      "Step: 56900 loss: 0.0010899548290936333\n",
      "Step: 57000 loss: 0.0006903733978555859\n",
      "Step: 57100 loss: 0.0012431587490064544\n",
      "Step: 57200 loss: 0.00038772927304956404\n",
      "Step: 57300 loss: 0.0014100608531457692\n",
      "Step: 57400 loss: 0.0017424933016001898\n",
      "Step: 57500 loss: 0.0012723907399171354\n",
      "Step: 57600 loss: 0.0010147825423734368\n",
      "Step: 57700 loss: 0.001732502638501532\n",
      "Step: 57800 loss: 0.0016379676131239717\n",
      "Step: 57900 loss: 0.0010932074229005428\n",
      "Step: 58000 loss: 0.0006935086013683356\n",
      "Step: 58100 loss: 0.0009559597949561338\n",
      "Step: 58200 loss: 0.0006492128903383154\n",
      "Step: 58300 loss: 0.0001048428840711857\n",
      "Step: 58400 loss: 0.000645927558098558\n",
      "Step: 58500 loss: 0.00015806206754576113\n",
      "Step: 58600 loss: 0.0008444676831883769\n",
      "Step: 58700 loss: 0.0010240673170713776\n",
      "Step: 58800 loss: 0.0002046134479570405\n",
      "Step: 58900 loss: 0.001035647586124684\n",
      "Step: 59000 loss: 3.736128882319001e-05\n",
      "Step: 59100 loss: 0.00028994385847065375\n",
      "Step: 59200 loss: 0.00018758334392218856\n",
      "Step: 59300 loss: 0.0005685695785643574\n",
      "Step: 59400 loss: 0.0016250423771801792\n",
      "Step: 59500 loss: 0.001053001251347503\n",
      "Step: 59600 loss: 6.0225121843737736e-05\n",
      "Step: 59700 loss: 0.0007975422832186308\n",
      "Step: 59800 loss: 0.0031815532336399955\n",
      "Step: 59900 loss: 8.364963457391461e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ai_conv_kernel = 128\n",
    "ai = AI(ai_conv_kernel)\n",
    "\n",
    "max_mem = 1000\n",
    "a = 28\n",
    "\n",
    "inp = []\n",
    "for i in range(max_mem):\n",
    "    inp.append(np.random.randn(1, a, 28))\n",
    "    \n",
    "criterion = nn.MSELoss()#SmoothL1Loss()MSELoss\n",
    "optimizer = optim.RMSprop(ai.parameters(), lr=0.00005)\n",
    "\n",
    "losses = 0\n",
    "mul = np.ones([1,28,10])\n",
    "mul = T.from_numpy(mul).to(ai.device)\n",
    "for j in range(1):\n",
    "    for i in range(60000):#train_images.shape[0]):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            inp1 = train_images[i]#inp[i % max_mem].copy() + np.random.randn(1, a, 28) * 0.1\n",
    "            inp1.shape = (1,1,28,28)\n",
    "            inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "            out = ai(inp1)\n",
    "    #        print(out)\n",
    "    #        print(target)\n",
    "            target = out.cpu().detach().numpy().copy()\n",
    "#            print(target.shape)\n",
    "#            target[0][8 - 1] = np.zeros(10)\n",
    "#            target[0][8 - 1][train_labels[i]] = 1.0\n",
    "\n",
    "#            target[0] = np.zeros([1,8,10])\n",
    "            n = np.zeros(10)\n",
    "            n[train_labels[i]] = 1.0\n",
    "            for o in range(ai_conv_kernel):\n",
    "                oo = (o + 1) / ai_conv_kernel\n",
    "                oo = oo * oo\n",
    "                target[0][o] = oo * n + (1.0 - oo) * target[0][o]\n",
    "#            target[0][8 - 1][train_labels[i]] = 1.0\n",
    "#            print(target)\n",
    "            target = T.from_numpy(np.array(target)).to(ai.device)\n",
    "            loss = criterion(out, target.float())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        losses = losses + loss.item()\n",
    "        if (i % 100) == 0:\n",
    "            print('Step: ' + str(i) + ' loss: ' + str(losses / 100))\n",
    "            losses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 0.9512\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(10000):#train_images.shape[0]):\n",
    "    optimizer.zero_grad()\n",
    "    inp1 = test_images[i]#inp[i % max_mem].copy()\n",
    "    inp1.shape = (1,1,28,28)\n",
    "    inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "    out = ai(inp1).cpu().detach().numpy()\n",
    "    m = np.argmax(out[0][5 - 1])\n",
    "    if m == test_labels[i]:\n",
    "#        print(m)\n",
    "        count = count + 1\n",
    "print(\"correct: \" + str(count / 10000))#train_images.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
