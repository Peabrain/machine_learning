{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AI, self).__init__()\n",
    "        self.lstm_layers = 1\n",
    "        self.lstm_depth = 1000\n",
    "        self.cnn1 = nn.Conv2d(1,128,5,stride=2,padding=1)\n",
    "        self.cnn2 = nn.Conv2d(128,128,3,stride=2,padding=0)\n",
    "        self.rnn = nn.LSTM(6 * 6, self.lstm_depth,self.lstm_layers)\n",
    "        self.fc1 = nn.Linear(self.lstm_depth,self.lstm_depth)\n",
    "        self.fc2 = nn.Linear(self.lstm_depth,10)\n",
    "        self.out = nn.Dropout(0.9)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')     \n",
    "        self.to(self.device)\n",
    "    def forward(self,x):\n",
    "        hx = T.zeros(self.lstm_layers,128, self.lstm_depth).to(self.device)\n",
    "        cx = T.zeros(self.lstm_layers,128, self.lstm_depth).to(self.device)\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "#        print(x.shape)\n",
    "        x = x.view(-1,128,6 * 6)\n",
    "        hx, cx = self.rnn(x, (hx, cx))\n",
    "        o = T.tanh(self.fc1(hx))\n",
    "        o = T.sigmoid(self.fc2(o))\n",
    "#        o = self.out(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLAYFIELD():\n",
    "    def __init__(self,width,height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pf = np.zeros((self.width,self.height,2))\n",
    "    def setStone(self,x,y,stone):\n",
    "        s = np.zeros(2)\n",
    "        s[stone] = 1\n",
    "        self.pf[y][x] = s\n",
    "    def get(self):\n",
    "        return self.pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:2: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-4-2c1d2a007f54>:4: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "with open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "with open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_labels = extract_labels(f)\n",
    "\n",
    "with open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "with open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = extract_labels(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 loss: 1.9638673402369024e-05\n",
      "Step: 100 loss: 0.0007004756139940582\n",
      "Step: 200 loss: 0.0005376565999176819\n",
      "Step: 300 loss: 0.0004265303796273656\n",
      "Step: 400 loss: 0.0003312422920498648\n",
      "Step: 500 loss: 0.0003355038977315417\n",
      "Step: 600 loss: 0.00034286423000594367\n",
      "Step: 700 loss: 0.00030643920774309664\n",
      "Step: 800 loss: 0.00026470941334991947\n",
      "Step: 900 loss: 0.0002709677997881954\n",
      "Step: 1000 loss: 0.00022747866154531948\n",
      "Step: 1100 loss: 0.0002688650946856797\n",
      "Step: 1200 loss: 0.00022410556820432248\n",
      "Step: 1300 loss: 0.0002420017381245998\n",
      "Step: 1400 loss: 0.000204765021435378\n",
      "Step: 1500 loss: 0.00018194442011918\n",
      "Step: 1600 loss: 0.00018101563234267814\n",
      "Step: 1700 loss: 0.00014436505884987127\n",
      "Step: 1800 loss: 0.0001170555394674011\n",
      "Step: 1900 loss: 0.00016016175487948203\n",
      "Step: 2000 loss: 0.0001536472590669291\n",
      "Step: 2100 loss: 0.00015528825632827648\n",
      "Step: 2200 loss: 0.00011827979105873964\n",
      "Step: 2300 loss: 0.00012412154587991607\n",
      "Step: 2400 loss: 0.00012199567477637174\n",
      "Step: 2500 loss: 0.00014905342418387592\n",
      "Step: 2600 loss: 0.00010788262511596258\n",
      "Step: 2700 loss: 0.00013482457464306208\n",
      "Step: 2800 loss: 0.0001498658404784692\n",
      "Step: 2900 loss: 0.0001316587142764547\n",
      "Step: 3000 loss: 0.00013751053805322044\n",
      "Step: 3100 loss: 0.0001441447551133024\n",
      "Step: 3200 loss: 7.987669254816865e-05\n",
      "Step: 3300 loss: 0.00010074069076836167\n",
      "Step: 3400 loss: 9.792735785595141e-05\n",
      "Step: 3500 loss: 8.80590038536866e-05\n",
      "Step: 3600 loss: 0.0001099073165187292\n",
      "Step: 3700 loss: 0.00011841167646025496\n",
      "Step: 3800 loss: 0.00011097742541664957\n",
      "Step: 3900 loss: 9.425085044767912e-05\n",
      "Step: 4000 loss: 0.00011174124999342894\n",
      "Step: 4100 loss: 0.00010033582855157874\n",
      "Step: 4200 loss: 0.00013557618021764028\n",
      "Step: 4300 loss: 9.195881033292607e-05\n",
      "Step: 4400 loss: 0.00011247029395235586\n",
      "Step: 4500 loss: 9.20136614030298e-05\n",
      "Step: 4600 loss: 8.53694005604666e-05\n",
      "Step: 4700 loss: 0.000126127557583402\n",
      "Step: 4800 loss: 0.00010503875359745508\n",
      "Step: 4900 loss: 9.003946479197112e-05\n",
      "Step: 5000 loss: 0.00010686249203615716\n",
      "Step: 5100 loss: 8.116242658275041e-05\n",
      "Step: 5200 loss: 0.0001653710667847008\n",
      "Step: 5300 loss: 7.457819373144048e-05\n",
      "Step: 5400 loss: 0.00011761835670711207\n",
      "Step: 5500 loss: 8.088058667510723e-05\n",
      "Step: 5600 loss: 0.00012233838209965597\n",
      "Step: 5700 loss: 9.47520279274272e-05\n",
      "Step: 5800 loss: 0.00011739020802480127\n",
      "Step: 5900 loss: 9.538791962569348e-05\n",
      "Step: 6000 loss: 6.522658978553863e-05\n",
      "Step: 6100 loss: 4.9691860410518985e-05\n",
      "Step: 6200 loss: 5.065626940307766e-05\n",
      "Step: 6300 loss: 0.00011603078151551926\n",
      "Step: 6400 loss: 4.748111899658625e-05\n",
      "Step: 6500 loss: 0.00010655077539070134\n",
      "Step: 6600 loss: 7.731296383425956e-05\n",
      "Step: 6700 loss: 8.989085959655085e-05\n",
      "Step: 6800 loss: 5.6993634625257525e-05\n",
      "Step: 6900 loss: 0.00015540281968327464\n",
      "Step: 7000 loss: 0.0001230127047881524\n",
      "Step: 7100 loss: 0.0001224195429045949\n",
      "Step: 7200 loss: 0.00011563994014466062\n",
      "Step: 7300 loss: 0.00013843061249986023\n",
      "Step: 7400 loss: 9.766365358814255e-05\n",
      "Step: 7500 loss: 7.617702920001079e-05\n",
      "Step: 7600 loss: 9.152260167411442e-05\n",
      "Step: 7700 loss: 9.534472381076852e-05\n",
      "Step: 7800 loss: 0.00010910258421006346\n",
      "Step: 7900 loss: 0.00012161532760927684\n",
      "Step: 8000 loss: 9.653221882871321e-05\n",
      "Step: 8100 loss: 4.670815989982202e-05\n",
      "Step: 8200 loss: 0.00012018207711491868\n",
      "Step: 8300 loss: 0.00015029281448917687\n",
      "Step: 8400 loss: 6.0614867585400134e-05\n",
      "Step: 8500 loss: 9.985629675128393e-05\n",
      "Step: 8600 loss: 9.187746758286152e-05\n",
      "Step: 8700 loss: 0.00014015443336830912\n",
      "Step: 8800 loss: 0.0001808661403541123\n",
      "Step: 8900 loss: 0.00013574342578039024\n",
      "Step: 9000 loss: 0.00010664812902469123\n",
      "Step: 9100 loss: 5.106724774854854e-05\n",
      "Step: 9200 loss: 9.018269080740993e-05\n",
      "Step: 9300 loss: 7.496984864289402e-05\n",
      "Step: 9400 loss: 9.417120562602577e-05\n",
      "Step: 9500 loss: 0.00011001993065221427\n",
      "Step: 9600 loss: 9.357726828682189e-05\n",
      "Step: 9700 loss: 5.281750589340106e-05\n",
      "Step: 9800 loss: 8.107262618295863e-05\n",
      "Step: 9900 loss: 6.273297451009797e-05\n",
      "Step: 10000 loss: 4.588155276280759e-05\n",
      "Step: 10100 loss: 0.0001013384018703789\n",
      "Step: 10200 loss: 9.873235881727283e-05\n",
      "Step: 10300 loss: 0.00013714892396990308\n",
      "Step: 10400 loss: 6.036293365113465e-05\n",
      "Step: 10500 loss: 5.849747241924774e-05\n",
      "Step: 10600 loss: 4.308707231018616e-05\n",
      "Step: 10700 loss: 5.9541985325779475e-05\n",
      "Step: 10800 loss: 0.0001234489486088819\n",
      "Step: 10900 loss: 6.314948338143723e-05\n",
      "Step: 11000 loss: 7.300196254838909e-05\n",
      "Step: 11100 loss: 8.474797262479683e-05\n",
      "Step: 11200 loss: 5.0732899154937174e-05\n",
      "Step: 11300 loss: 9.176340075057965e-05\n",
      "Step: 11400 loss: 7.217892904527901e-05\n",
      "Step: 11500 loss: 8.139541507635073e-05\n",
      "Step: 11600 loss: 0.00012024180771106785\n",
      "Step: 11700 loss: 7.451894609605602e-05\n",
      "Step: 11800 loss: 8.486966570877996e-05\n",
      "Step: 11900 loss: 8.957360822833493e-05\n",
      "Step: 12000 loss: 6.775989952728878e-05\n",
      "Step: 12100 loss: 5.6259700901328816e-05\n",
      "Step: 12200 loss: 3.921983468785584e-05\n",
      "Step: 12300 loss: 9.852175414263087e-05\n",
      "Step: 12400 loss: 7.04376696708664e-05\n",
      "Step: 12500 loss: 0.0001131751518396129\n",
      "Step: 12600 loss: 0.00010129158967629337\n",
      "Step: 12700 loss: 0.0001283955595205555\n",
      "Step: 12800 loss: 8.484977258142123e-05\n",
      "Step: 12900 loss: 8.717561150731967e-05\n",
      "Step: 13000 loss: 0.00010399773170554738\n",
      "Step: 13100 loss: 0.0001624044502750621\n",
      "Step: 13200 loss: 8.259769183467825e-05\n",
      "Step: 13300 loss: 3.39448260282893e-05\n",
      "Step: 13400 loss: 9.077580282863096e-05\n",
      "Step: 13500 loss: 6.217274214485257e-05\n",
      "Step: 13600 loss: 5.406793812131383e-05\n",
      "Step: 13700 loss: 8.809436254338721e-05\n",
      "Step: 13800 loss: 0.00011050602193165559\n",
      "Step: 13900 loss: 6.610714307939247e-05\n",
      "Step: 14000 loss: 9.880811994349869e-05\n",
      "Step: 14100 loss: 8.660507188778865e-05\n",
      "Step: 14200 loss: 6.606721929593107e-05\n",
      "Step: 14300 loss: 8.550345122973368e-05\n",
      "Step: 14400 loss: 0.00014514295245774634\n",
      "Step: 14500 loss: 5.983833428679475e-05\n",
      "Step: 14600 loss: 7.612243511019301e-05\n",
      "Step: 14700 loss: 0.00013984885017965355\n",
      "Step: 14800 loss: 0.00013264263407627652\n",
      "Step: 14900 loss: 6.446516894691001e-05\n",
      "Step: 15000 loss: 5.1653618017510894e-05\n",
      "Step: 15100 loss: 3.898732527190418e-05\n",
      "Step: 15200 loss: 0.00012413266218000985\n",
      "Step: 15300 loss: 4.7743843022249874e-05\n",
      "Step: 15400 loss: 6.972131893899558e-05\n",
      "Step: 15500 loss: 8.519536202939549e-05\n",
      "Step: 15600 loss: 4.789335107659553e-05\n",
      "Step: 15700 loss: 4.1176639863254036e-05\n",
      "Step: 15800 loss: 0.0001474829872882566\n",
      "Step: 15900 loss: 0.00010470999797551172\n",
      "Step: 16000 loss: 6.152587267976628e-05\n",
      "Step: 16100 loss: 0.00011200399961587948\n",
      "Step: 16200 loss: 7.691729151986238e-05\n",
      "Step: 16300 loss: 4.8870951240829186e-05\n",
      "Step: 16400 loss: 5.109016596222205e-05\n",
      "Step: 16500 loss: 6.334061091447297e-05\n",
      "Step: 16600 loss: 6.553211436450823e-05\n",
      "Step: 16700 loss: 8.623206488294954e-05\n",
      "Step: 16800 loss: 0.00010891679811834364\n",
      "Step: 16900 loss: 5.0405542850811004e-05\n",
      "Step: 17000 loss: 9.046461291784879e-05\n",
      "Step: 17100 loss: 8.774600811211287e-05\n",
      "Step: 17200 loss: 6.172890253060004e-05\n",
      "Step: 17300 loss: 6.760738294214974e-05\n",
      "Step: 17400 loss: 2.9642792160942123e-05\n",
      "Step: 17500 loss: 9.745607892845954e-05\n",
      "Step: 17600 loss: 0.00010139435325953627\n",
      "Step: 17700 loss: 7.777924314144258e-05\n",
      "Step: 17800 loss: 0.0001055264520589283\n",
      "Step: 17900 loss: 0.00010618140403067855\n",
      "Step: 18000 loss: 6.210121491569076e-05\n",
      "Step: 18100 loss: 7.261698697808683e-05\n",
      "Step: 18200 loss: 4.7392442080873566e-05\n",
      "Step: 18300 loss: 4.633438595281092e-05\n",
      "Step: 18400 loss: 6.306139330668259e-05\n",
      "Step: 18500 loss: 5.315028623925855e-05\n",
      "Step: 18600 loss: 4.630881013969912e-05\n",
      "Step: 18700 loss: 3.941392778729735e-05\n",
      "Step: 18800 loss: 7.813944703205066e-05\n",
      "Step: 18900 loss: 3.976111208448429e-05\n",
      "Step: 19000 loss: 1.74962374791221e-05\n",
      "Step: 19100 loss: 5.9384145806369304e-05\n",
      "Step: 19200 loss: 7.202530642217652e-05\n",
      "Step: 19300 loss: 7.642123222050046e-05\n",
      "Step: 19400 loss: 8.266743406499799e-05\n",
      "Step: 19500 loss: 5.5937533848382516e-05\n",
      "Step: 19600 loss: 5.858253750025355e-05\n",
      "Step: 19700 loss: 2.658146573809361e-05\n",
      "Step: 19800 loss: 4.519885526170775e-05\n",
      "Step: 19900 loss: 7.063189500974509e-05\n",
      "Step: 20000 loss: 3.985408453273109e-05\n",
      "Step: 20100 loss: 0.00010300681370736253\n",
      "Step: 20200 loss: 8.616806108775243e-05\n",
      "Step: 20300 loss: 9.878799819352579e-05\n",
      "Step: 20400 loss: 5.05811607634854e-05\n",
      "Step: 20500 loss: 4.25603727973467e-05\n",
      "Step: 20600 loss: 6.505890903062195e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20700 loss: 9.52728726983132e-05\n",
      "Step: 20800 loss: 0.00012032320454645529\n",
      "Step: 20900 loss: 7.165671039063959e-05\n",
      "Step: 21000 loss: 0.0001048702426290049\n",
      "Step: 21100 loss: 6.0942875899110404e-05\n",
      "Step: 21200 loss: 5.8998262617793173e-05\n",
      "Step: 21300 loss: 6.0858318921743756e-05\n",
      "Step: 21400 loss: 7.077841009170527e-05\n",
      "Step: 21500 loss: 6.791051914708745e-05\n",
      "Step: 21600 loss: 5.177772389643564e-05\n",
      "Step: 21700 loss: 5.760674571440916e-05\n",
      "Step: 21800 loss: 2.3645362372857193e-05\n",
      "Step: 21900 loss: 2.1310883492811073e-05\n",
      "Step: 22000 loss: 7.8488156458274e-05\n",
      "Step: 22100 loss: 3.7347741937256986e-05\n",
      "Step: 22200 loss: 0.00012649852589868615\n",
      "Step: 22300 loss: 8.976927567952764e-05\n",
      "Step: 22400 loss: 2.177084785008887e-05\n",
      "Step: 22500 loss: 0.0001231174717775385\n",
      "Step: 22600 loss: 0.0001196998423952067\n",
      "Step: 22700 loss: 8.935853692513263e-05\n",
      "Step: 22800 loss: 8.135812164420387e-05\n",
      "Step: 22900 loss: 7.201336947272096e-05\n",
      "Step: 23000 loss: 1.3525111050016747e-05\n",
      "Step: 23100 loss: 4.8516565111880184e-05\n",
      "Step: 23200 loss: 5.5415797569349225e-05\n",
      "Step: 23300 loss: 4.089964449415007e-05\n",
      "Step: 23400 loss: 3.793852090061556e-05\n",
      "Step: 23500 loss: 6.25403210819897e-05\n",
      "Step: 23600 loss: 5.670776281656131e-05\n",
      "Step: 23700 loss: 8.160088612453542e-05\n",
      "Step: 23800 loss: 0.00010960524626677071\n",
      "Step: 23900 loss: 5.8052839831539415e-05\n",
      "Step: 24000 loss: 6.270767605654371e-05\n",
      "Step: 24100 loss: 7.60589730158756e-05\n",
      "Step: 24200 loss: 6.49907143145434e-05\n",
      "Step: 24300 loss: 0.00010878435613715709\n",
      "Step: 24400 loss: 1.851757055172598e-05\n",
      "Step: 24500 loss: 5.843287167515143e-05\n",
      "Step: 24600 loss: 6.0646466796292485e-05\n",
      "Step: 24700 loss: 0.00011667259023834476\n",
      "Step: 24800 loss: 9.383502333992766e-05\n",
      "Step: 24900 loss: 5.002681867047443e-05\n",
      "Step: 25000 loss: 6.136297998136087e-05\n",
      "Step: 25100 loss: 4.9809187138025735e-05\n",
      "Step: 25200 loss: 6.028793923025688e-05\n",
      "Step: 25300 loss: 6.670779713773279e-05\n",
      "Step: 25400 loss: 4.4997251896230494e-05\n",
      "Step: 25500 loss: 4.585665273215689e-05\n",
      "Step: 25600 loss: 5.7220842102667645e-05\n",
      "Step: 25700 loss: 7.851280441943942e-05\n",
      "Step: 25800 loss: 6.55524843605404e-05\n",
      "Step: 25900 loss: 5.7356608923457844e-05\n",
      "Step: 26000 loss: 6.410372531018282e-05\n",
      "Step: 26100 loss: 4.640120006555559e-05\n",
      "Step: 26200 loss: 3.3939650249292794e-05\n",
      "Step: 26300 loss: 6.756738140452078e-05\n",
      "Step: 26400 loss: 9.236404187906722e-05\n",
      "Step: 26500 loss: 6.157976786201224e-05\n",
      "Step: 26600 loss: 6.97385403455364e-05\n",
      "Step: 26700 loss: 7.837676230260992e-05\n",
      "Step: 26800 loss: 0.00013754417939702002\n",
      "Step: 26900 loss: 8.868395085524838e-05\n",
      "Step: 27000 loss: 2.9763362382180246e-05\n",
      "Step: 27100 loss: 5.449838261871509e-05\n",
      "Step: 27200 loss: 9.769498418234602e-05\n",
      "Step: 27300 loss: 5.286093315244322e-05\n",
      "Step: 27400 loss: 2.2656628903114927e-05\n",
      "Step: 27500 loss: 6.732014807732334e-05\n",
      "Step: 27600 loss: 6.747905162246993e-05\n",
      "Step: 27700 loss: 9.71403353694189e-05\n",
      "Step: 27800 loss: 7.531441409561967e-05\n",
      "Step: 27900 loss: 4.295303381820936e-05\n",
      "Step: 28000 loss: 3.4522763587814255e-05\n",
      "Step: 28100 loss: 3.655776492821783e-05\n",
      "Step: 28200 loss: 9.948990767564502e-05\n",
      "Step: 28300 loss: 1.042826776691541e-05\n",
      "Step: 28400 loss: 7.02563799608824e-05\n",
      "Step: 28500 loss: 3.170510783052416e-05\n",
      "Step: 28600 loss: 7.415476790098463e-05\n",
      "Step: 28700 loss: 0.00010853370003781838\n",
      "Step: 28800 loss: 6.0262335389762444e-05\n",
      "Step: 28900 loss: 4.283729249372037e-05\n",
      "Step: 29000 loss: 4.239190930241054e-05\n",
      "Step: 29100 loss: 6.486680444042214e-05\n",
      "Step: 29200 loss: 7.758541934951069e-05\n",
      "Step: 29300 loss: 6.177855921806863e-05\n",
      "Step: 29400 loss: 0.00011281568133120868\n",
      "Step: 29500 loss: 4.8722791822521924e-05\n",
      "Step: 29600 loss: 4.9809600080066296e-05\n",
      "Step: 29700 loss: 2.64485430446304e-05\n",
      "Step: 29800 loss: 7.80144713197406e-05\n",
      "Step: 29900 loss: 6.980890555988495e-05\n",
      "Step: 30000 loss: 7.970815053755409e-05\n",
      "Step: 30100 loss: 4.990892359174381e-05\n",
      "Step: 30200 loss: 8.24038124156523e-05\n",
      "Step: 30300 loss: 4.240046584580881e-05\n",
      "Step: 30400 loss: 4.1972215092078714e-05\n",
      "Step: 30500 loss: 5.6286223243677826e-05\n",
      "Step: 30600 loss: 4.756384554284842e-05\n",
      "Step: 30700 loss: 8.729285376693596e-05\n",
      "Step: 30800 loss: 5.723791311313065e-05\n",
      "Step: 30900 loss: 8.576118761179252e-05\n",
      "Step: 31000 loss: 7.076841782276189e-05\n",
      "Step: 31100 loss: 4.536979538153574e-05\n",
      "Step: 31200 loss: 9.28350009401857e-05\n",
      "Step: 31300 loss: 7.198734488016668e-05\n",
      "Step: 31400 loss: 8.515384340291199e-05\n",
      "Step: 31500 loss: 9.402415749585557e-05\n",
      "Step: 31600 loss: 7.809273242608938e-05\n",
      "Step: 31700 loss: 7.257865459588064e-05\n",
      "Step: 31800 loss: 0.00010942589732643171\n",
      "Step: 31900 loss: 5.053737994206386e-05\n",
      "Step: 32000 loss: 6.400946663482898e-05\n",
      "Step: 32100 loss: 9.345272003735694e-05\n",
      "Step: 32200 loss: 7.680444625401294e-05\n",
      "Step: 32300 loss: 5.6547763466143766e-05\n",
      "Step: 32400 loss: 6.344440025084719e-05\n",
      "Step: 32500 loss: 8.922820078832494e-05\n",
      "Step: 32600 loss: 3.097984637685292e-05\n",
      "Step: 32700 loss: 3.585856103085927e-05\n",
      "Step: 32800 loss: 4.6181233288786764e-05\n",
      "Step: 32900 loss: 6.392623193595349e-05\n",
      "Step: 33000 loss: 3.926130192884791e-05\n",
      "Step: 33100 loss: 4.426351794243333e-05\n",
      "Step: 33200 loss: 2.4358987000789868e-05\n",
      "Step: 33300 loss: 2.181602784854686e-05\n",
      "Step: 33400 loss: 7.568747633630136e-05\n",
      "Step: 33500 loss: 4.959791020996418e-05\n",
      "Step: 33600 loss: 5.3850074350995095e-05\n",
      "Step: 33700 loss: 5.0336362076484064e-05\n",
      "Step: 33800 loss: 5.7834096191615724e-05\n",
      "Step: 33900 loss: 6.117100021715061e-05\n",
      "Step: 34000 loss: 1.8186658415488832e-05\n",
      "Step: 34100 loss: 6.116129486327893e-05\n",
      "Step: 34200 loss: 2.8629769885526458e-05\n",
      "Step: 34300 loss: 2.5820204200970308e-05\n",
      "Step: 34400 loss: 5.303867273128038e-05\n",
      "Step: 34500 loss: 0.00010071461804601389\n",
      "Step: 34600 loss: 6.756924554206512e-05\n",
      "Step: 34700 loss: 9.407305011298606e-05\n",
      "Step: 34800 loss: 7.664911095664806e-05\n",
      "Step: 34900 loss: 8.385046123813744e-05\n",
      "Step: 35000 loss: 5.1120162647282366e-05\n",
      "Step: 35100 loss: 4.533776209007634e-05\n",
      "Step: 35200 loss: 4.129328437077362e-05\n",
      "Step: 35300 loss: 5.5328351966316135e-05\n",
      "Step: 35400 loss: 6.331632552079469e-05\n",
      "Step: 35500 loss: 8.161512827732587e-05\n",
      "Step: 35600 loss: 2.465528535806616e-05\n",
      "Step: 35700 loss: 9.225376190964196e-05\n",
      "Step: 35800 loss: 4.428102858003324e-05\n",
      "Step: 35900 loss: 4.4335455988200325e-05\n",
      "Step: 36000 loss: 3.893423218393033e-05\n",
      "Step: 36100 loss: 3.849129758314751e-05\n",
      "Step: 36200 loss: 7.095340340524281e-05\n",
      "Step: 36300 loss: 4.050864951141708e-05\n",
      "Step: 36400 loss: 3.650277656034096e-05\n",
      "Step: 36500 loss: 7.365131630781451e-05\n",
      "Step: 36600 loss: 5.805462680878115e-05\n",
      "Step: 36700 loss: 1.9004912560864897e-05\n",
      "Step: 36800 loss: 6.057804584574278e-05\n",
      "Step: 36900 loss: 8.733809076696408e-05\n",
      "Step: 37000 loss: 3.9415062845109536e-05\n",
      "Step: 37100 loss: 9.085655992481456e-05\n",
      "Step: 37200 loss: 8.756707220720195e-05\n",
      "Step: 37300 loss: 4.946216564616579e-05\n",
      "Step: 37400 loss: 6.817475317161148e-05\n",
      "Step: 37500 loss: 7.245489300795161e-05\n",
      "Step: 37600 loss: 6.158614003711094e-05\n",
      "Step: 37700 loss: 3.885467973367085e-05\n",
      "Step: 37800 loss: 5.768125538192059e-05\n",
      "Step: 37900 loss: 5.6906370147677964e-05\n",
      "Step: 38000 loss: 4.866287768724664e-05\n",
      "Step: 38100 loss: 4.15543354169845e-05\n",
      "Step: 38200 loss: 5.108196319905778e-05\n",
      "Step: 38300 loss: 6.0507606220538966e-05\n",
      "Step: 38400 loss: 7.935076855183353e-05\n",
      "Step: 38500 loss: 3.196836460543917e-05\n",
      "Step: 38600 loss: 6.613754598212784e-05\n",
      "Step: 38700 loss: 9.365214560987933e-05\n",
      "Step: 38800 loss: 7.079435040630356e-05\n",
      "Step: 38900 loss: 8.69836326704787e-06\n",
      "Step: 39000 loss: 3.784923154298903e-05\n",
      "Step: 39100 loss: 1.257247291795327e-05\n",
      "Step: 39200 loss: 7.530938756548866e-05\n",
      "Step: 39300 loss: 6.0455273810184896e-05\n",
      "Step: 39400 loss: 0.00013397211429401866\n",
      "Step: 39500 loss: 5.982493453256854e-05\n",
      "Step: 39600 loss: 3.706185994157174e-05\n",
      "Step: 39700 loss: 6.857711141285083e-05\n",
      "Step: 39800 loss: 5.141796652999631e-05\n",
      "Step: 39900 loss: 8.117496052950201e-05\n",
      "Step: 40000 loss: 7.961424410621865e-05\n",
      "Step: 40100 loss: 5.002949631990994e-05\n",
      "Step: 40200 loss: 4.7701214850435216e-05\n",
      "Step: 40300 loss: 6.65860311641997e-05\n",
      "Step: 40400 loss: 4.925385803091986e-05\n",
      "Step: 40500 loss: 2.9237932512362618e-05\n",
      "Step: 40600 loss: 6.019445957603065e-05\n",
      "Step: 40700 loss: 7.480431917794183e-05\n",
      "Step: 40800 loss: 4.6359979555438134e-05\n",
      "Step: 40900 loss: 8.897752429332329e-05\n",
      "Step: 41000 loss: 6.071357681806999e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41100 loss: 7.802137523651497e-05\n",
      "Step: 41200 loss: 2.4087464209276987e-05\n",
      "Step: 41300 loss: 8.366986054001102e-05\n",
      "Step: 41400 loss: 7.611611990824496e-05\n",
      "Step: 41500 loss: 7.245426083707596e-05\n",
      "Step: 41600 loss: 6.632443890121742e-05\n",
      "Step: 41700 loss: 4.1100504417765914e-05\n",
      "Step: 41800 loss: 5.56182418838036e-05\n",
      "Step: 41900 loss: 2.607720473967845e-05\n",
      "Step: 42000 loss: 8.020968543913876e-05\n",
      "Step: 42100 loss: 5.0018822789492654e-05\n",
      "Step: 42200 loss: 4.818659567442207e-05\n",
      "Step: 42300 loss: 2.3966166835425185e-05\n",
      "Step: 42400 loss: 6.731133493913855e-05\n",
      "Step: 42500 loss: 0.00010733863964531243\n",
      "Step: 42600 loss: 3.984957869584527e-05\n",
      "Step: 42700 loss: 5.521971187022245e-05\n",
      "Step: 42800 loss: 3.729451919753712e-05\n",
      "Step: 42900 loss: 6.504462399719247e-05\n",
      "Step: 43000 loss: 7.629647258866056e-05\n",
      "Step: 43100 loss: 7.39712824751182e-05\n",
      "Step: 43200 loss: 5.679566830710025e-05\n",
      "Step: 43300 loss: 6.289838543783899e-05\n",
      "Step: 43400 loss: 1.4398970742892647e-05\n",
      "Step: 43500 loss: 4.6529064986700064e-05\n",
      "Step: 43600 loss: 6.302779012190451e-05\n",
      "Step: 43700 loss: 3.6457742184053954e-05\n",
      "Step: 43800 loss: 2.4811917646201387e-05\n",
      "Step: 43900 loss: 5.9546621775236064e-05\n",
      "Step: 44000 loss: 5.57021163789484e-05\n",
      "Step: 44100 loss: 5.523802651469012e-05\n",
      "Step: 44200 loss: 6.0028066502875176e-05\n",
      "Step: 44300 loss: 6.070904718846304e-05\n",
      "Step: 44400 loss: 3.800575008772289e-05\n",
      "Step: 44500 loss: 7.110696206118927e-05\n",
      "Step: 44600 loss: 2.2442694859012847e-05\n",
      "Step: 44700 loss: 4.468586716923184e-05\n",
      "Step: 44800 loss: 6.736921133704499e-05\n",
      "Step: 44900 loss: 7.527723562130273e-05\n",
      "Step: 45000 loss: 9.045799774541896e-05\n",
      "Step: 45100 loss: 7.007455892628745e-05\n",
      "Step: 45200 loss: 4.769303006518788e-05\n",
      "Step: 45300 loss: 4.7524202330417876e-05\n",
      "Step: 45400 loss: 2.2708881987304473e-05\n",
      "Step: 45500 loss: 8.497903920849703e-05\n",
      "Step: 45600 loss: 8.207101769643344e-05\n",
      "Step: 45700 loss: 6.380847698641556e-05\n",
      "Step: 45800 loss: 5.992972034747135e-05\n",
      "Step: 45900 loss: 5.2695692470998704e-05\n",
      "Step: 46000 loss: 6.958907144238746e-05\n",
      "Step: 46100 loss: 8.277914948413124e-05\n",
      "Step: 46200 loss: 7.654932931118241e-05\n",
      "Step: 46300 loss: 8.412521856595578e-05\n",
      "Step: 46400 loss: 0.00011423414975249569\n",
      "Step: 46500 loss: 6.434764624068112e-05\n",
      "Step: 46600 loss: 2.8118089686046676e-05\n",
      "Step: 46700 loss: 5.3235295145178663e-05\n",
      "Step: 46800 loss: 8.488758172736532e-05\n",
      "Step: 46900 loss: 6.581308570733801e-05\n",
      "Step: 47000 loss: 4.4268832022480444e-05\n",
      "Step: 47100 loss: 6.207739547820057e-05\n",
      "Step: 47200 loss: 2.9944396831133034e-05\n",
      "Step: 47300 loss: 9.984914526650068e-05\n",
      "Step: 47400 loss: 6.652495571830341e-05\n",
      "Step: 47500 loss: 5.9187649656657904e-05\n",
      "Step: 47600 loss: 6.472931075858223e-05\n",
      "Step: 47700 loss: 6.314707109583928e-05\n",
      "Step: 47800 loss: 3.986657431100493e-05\n",
      "Step: 47900 loss: 3.9732404198425386e-05\n",
      "Step: 48000 loss: 7.843875658291476e-05\n",
      "Step: 48100 loss: 4.780399951775427e-05\n",
      "Step: 48200 loss: 3.8376740816410406e-05\n",
      "Step: 48300 loss: 4.613113689816428e-05\n",
      "Step: 48400 loss: 5.971059686622926e-05\n",
      "Step: 48500 loss: 2.7191678454207556e-05\n",
      "Step: 48600 loss: 5.055484684737044e-05\n",
      "Step: 48700 loss: 3.9562391371192885e-05\n",
      "Step: 48800 loss: 3.4987569303586155e-05\n",
      "Step: 48900 loss: 4.547399177673661e-05\n",
      "Step: 49000 loss: 0.0001020826031742883\n",
      "Step: 49100 loss: 0.00011088262935272986\n",
      "Step: 49200 loss: 4.279164661356916e-05\n",
      "Step: 49300 loss: 7.05391060515015e-05\n",
      "Step: 49400 loss: 3.918818004939317e-05\n",
      "Step: 49500 loss: 4.8303297742590126e-05\n",
      "Step: 49600 loss: 9.932258056589094e-05\n",
      "Step: 49700 loss: 6.32674468216976e-05\n",
      "Step: 49800 loss: 2.8847246798301417e-05\n",
      "Step: 49900 loss: 8.265255744162814e-05\n",
      "Step: 50000 loss: 3.863904127837881e-05\n",
      "Step: 50100 loss: 4.948182627724895e-05\n",
      "Step: 50200 loss: 4.109530822845292e-05\n",
      "Step: 50300 loss: 6.787714713412441e-05\n",
      "Step: 50400 loss: 0.00011286487232290421\n",
      "Step: 50500 loss: 9.903642340459706e-05\n",
      "Step: 50600 loss: 6.286447019658303e-05\n",
      "Step: 50700 loss: 3.253276853554199e-05\n",
      "Step: 50800 loss: 3.833106561894839e-05\n",
      "Step: 50900 loss: 2.5245348802618305e-05\n",
      "Step: 51000 loss: 3.624169313590331e-05\n",
      "Step: 51100 loss: 2.233677383514765e-05\n",
      "Step: 51200 loss: 2.2007978980110588e-05\n",
      "Step: 51300 loss: 5.6884288200862446e-05\n",
      "Step: 51400 loss: 5.066988781681436e-05\n",
      "Step: 51500 loss: 4.77285065948041e-05\n",
      "Step: 51600 loss: 3.897471988126011e-05\n",
      "Step: 51700 loss: 5.241542890408735e-05\n",
      "Step: 51800 loss: 3.446101420468972e-05\n",
      "Step: 51900 loss: 1.7263016794876186e-05\n",
      "Step: 52000 loss: 9.684247429545546e-05\n",
      "Step: 52100 loss: 4.466121635265913e-05\n",
      "Step: 52200 loss: 9.306543355306673e-05\n",
      "Step: 52300 loss: 6.77853784491933e-05\n",
      "Step: 52400 loss: 6.0490895084193144e-05\n",
      "Step: 52500 loss: 3.119942135694442e-05\n",
      "Step: 52600 loss: 1.7036819502441734e-05\n",
      "Step: 52700 loss: 3.668500016058118e-05\n",
      "Step: 52800 loss: 7.340239612896204e-05\n",
      "Step: 52900 loss: 8.206035923456811e-05\n",
      "Step: 53000 loss: 0.00014327635371427783\n",
      "Step: 53100 loss: 5.338319524737678e-05\n",
      "Step: 53200 loss: 7.817203425532271e-05\n",
      "Step: 53300 loss: 2.8033011575023804e-05\n",
      "Step: 53400 loss: 3.000568949631466e-05\n",
      "Step: 53500 loss: 4.636235458897408e-05\n",
      "Step: 53600 loss: 8.206314954480986e-05\n",
      "Step: 53700 loss: 5.7254677950019925e-05\n",
      "Step: 53800 loss: 3.85144759730735e-05\n",
      "Step: 53900 loss: 7.836318461347136e-05\n",
      "Step: 54000 loss: 7.827206385668583e-05\n",
      "Step: 54100 loss: 5.784982100395242e-05\n",
      "Step: 54200 loss: 2.9515692724055853e-05\n",
      "Step: 54300 loss: 1.9998831503184876e-05\n",
      "Step: 54400 loss: 2.9146830232863464e-05\n",
      "Step: 54500 loss: 1.4734516193728452e-05\n",
      "Step: 54600 loss: 5.521867152490312e-05\n",
      "Step: 54700 loss: 5.452503859527863e-05\n",
      "Step: 54800 loss: 3.388468267065514e-05\n",
      "Step: 54900 loss: 9.488862177908286e-05\n",
      "Step: 55000 loss: 5.783460947149099e-05\n",
      "Step: 55100 loss: 5.012598253803735e-05\n",
      "Step: 55200 loss: 5.966552781594858e-05\n",
      "Step: 55300 loss: 4.5874172839133476e-05\n",
      "Step: 55400 loss: 3.602472956263425e-05\n",
      "Step: 55500 loss: 4.4650145796614474e-05\n",
      "Step: 55600 loss: 3.7821903792152334e-05\n",
      "Step: 55700 loss: 4.593957794095049e-05\n",
      "Step: 55800 loss: 5.9249210564006614e-05\n",
      "Step: 55900 loss: 7.795626832221236e-05\n",
      "Step: 56000 loss: 3.579016810965785e-05\n",
      "Step: 56100 loss: 5.985953814361123e-05\n",
      "Step: 56200 loss: 4.044692521616933e-05\n",
      "Step: 56300 loss: 7.387692775462807e-05\n",
      "Step: 56400 loss: 4.853732514144338e-05\n",
      "Step: 56500 loss: 8.46467735474199e-05\n",
      "Step: 56600 loss: 4.6829634083962455e-05\n",
      "Step: 56700 loss: 4.498238002886179e-05\n",
      "Step: 56800 loss: 4.684394103069561e-05\n",
      "Step: 56900 loss: 4.831582542460494e-05\n",
      "Step: 57000 loss: 2.5649142704465833e-05\n",
      "Step: 57100 loss: 5.9074700686324585e-05\n",
      "Step: 57200 loss: 6.8340690014107034e-06\n",
      "Step: 57300 loss: 4.668218952858849e-05\n",
      "Step: 57400 loss: 5.3892584137937014e-05\n",
      "Step: 57500 loss: 3.040211010702093e-05\n",
      "Step: 57600 loss: 3.8983616200068404e-05\n",
      "Step: 57700 loss: 5.8423498080647994e-05\n",
      "Step: 57800 loss: 6.889440811940623e-05\n",
      "Step: 57900 loss: 3.659300322931658e-05\n",
      "Step: 58000 loss: 2.6703549641516334e-05\n",
      "Step: 58100 loss: 6.089992746988987e-05\n",
      "Step: 58200 loss: 1.7687744733365236e-05\n",
      "Step: 58300 loss: 9.448473010003622e-06\n",
      "Step: 58400 loss: 2.6242711674598353e-05\n",
      "Step: 58500 loss: 1.5315579783649613e-05\n",
      "Step: 58600 loss: 1.2210993229622991e-05\n",
      "Step: 58700 loss: 2.0328571404569507e-05\n",
      "Step: 58800 loss: 4.2595657592597865e-06\n",
      "Step: 58900 loss: 2.5956250624772915e-05\n",
      "Step: 59000 loss: 7.410258164648464e-06\n",
      "Step: 59100 loss: 7.769946231895585e-06\n",
      "Step: 59200 loss: 7.945385590953091e-07\n",
      "Step: 59300 loss: 2.4283288941518018e-05\n",
      "Step: 59400 loss: 3.7695769230470246e-05\n",
      "Step: 59500 loss: 5.1513303751211656e-05\n",
      "Step: 59600 loss: 5.821277834089367e-07\n",
      "Step: 59700 loss: 1.4031141053062468e-05\n",
      "Step: 59800 loss: 9.881719546561696e-05\n",
      "Step: 59900 loss: 4.2729984126022025e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ai = AI()\n",
    "\n",
    "max_mem = 1000\n",
    "a = 28\n",
    "\n",
    "inp = []\n",
    "for i in range(max_mem):\n",
    "    inp.append(np.random.randn(1, a, 28))\n",
    "    \n",
    "criterion = nn.MSELoss()#SmoothL1Loss()MSELoss\n",
    "optimizer = optim.RMSprop(ai.parameters(), lr=0.00005)\n",
    "\n",
    "losses = 0\n",
    "for j in range(1):\n",
    "    for i in range(60000):#train_images.shape[0]):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            inp1 = train_images[i]#inp[i % max_mem].copy() + np.random.randn(1, a, 28) * 0.1\n",
    "            inp1.shape = (1,1,28,28)\n",
    "            inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "            out = ai(inp1)\n",
    "    #        print(out)\n",
    "    #        print(target)\n",
    "            target = out.cpu().detach().numpy().copy()\n",
    "    #        print(target)\n",
    "            target[0][5 - 1] = np.zeros(10)\n",
    "            target[0][5 - 1][train_labels[i]] = 1.0\n",
    "    #        print(target)\n",
    "            target = T.from_numpy(np.array(target)).to(ai.device)\n",
    "            loss = criterion(out, target.float())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        losses = losses + loss.item()\n",
    "        if (i % 100) == 0:\n",
    "            print('Step: ' + str(i) + ' loss: ' + str(losses / 100))\n",
    "            losses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 0.9593\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(10000):#train_images.shape[0]):\n",
    "    optimizer.zero_grad()\n",
    "    inp1 = test_images[i]#inp[i % max_mem].copy()\n",
    "    inp1.shape = (1,1,28,28)\n",
    "    inp1 = T.from_numpy(inp1).float().to(ai.device)\n",
    "    out = ai(inp1).cpu().detach().numpy()\n",
    "    m = np.argmax(out[0][5 - 1])\n",
    "    if m == test_labels[i]:\n",
    "#        print(m)\n",
    "        count = count + 1\n",
    "print(\"correct: \" + str(count / 10000))#train_images.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
